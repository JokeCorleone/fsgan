{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FSGAN（conda版）.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1SZW9d7yu4Al",
        "XR5vQfn9s0op",
        "7wEeE1IsG2Sw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjandnn/fsgan/blob/master/FSGAN%EF%BC%88conda%E7%89%88%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE0fIrpau-nv",
        "colab_type": "text"
      },
      "source": [
        "# **FSGAN**\n",
        "作者：[Yuval Nirkin](https://github.com/YuvalNirkin)\n",
        "\n",
        "colab： jjandnn ， zhuhaozh ， wangchao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZW9d7yu4Al",
        "colab_type": "text"
      },
      "source": [
        "# 安装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95qjm6RkFX1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 必须 p100 \n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orFKdoubPfeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/sample_data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1L4H_tvXX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local\n",
        "!rm Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhQAt7jCxj_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 依赖，请勿修改安装顺序\n",
        "!sudo apt-get install build-essential cmake unzip pkg-config\n",
        "!sudo apt-get install libjpeg-dev libpng-dev \n",
        "!sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev \n",
        "!sudo apt-get install libxvidcore-dev libx264-dev\n",
        "!sudo apt-get install libatlas-base-dev gfortran\n",
        "!sudo apt-get install python3-dev\n",
        "!conda install numpy -y\n",
        "\n",
        "!sudo add-apt-repository ppa:jonathonf/ffmpeg-4\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install ffmpeg\n",
        "\n",
        "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch -y\n",
        "!conda install -c anaconda ipykernel -y\n",
        "!conda install -c conda-forge yacs -y\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install tensorboardX\n",
        "!pip install ffmpeg-python\n",
        "!pip install face_alignment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fToL6OKn2qaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1，云盘源码安装（首次需要，之后使用无需再装）\n",
        "\n",
        "%cd /content/drive/My Drive\n",
        "!mkdir -p fsganLab/fshome\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "\n",
        "!git clone https://github.com/YuvalNirkin/fsgan.git\n",
        "!git clone https://github.com/YuvalNirkin/face_detection_dsfd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNbgTpabri-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共17个模型文件\n",
        "# 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共17个模型文件\n",
        "# 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共17个模型文件\n",
        "\n",
        "\n",
        "!python download_fsgan_models.py -m v1\n",
        "!python download_fsgan_models.py -m v2\n",
        "# 由于 colab 空格问题，重新组织下目录结构\n",
        "!mv /content/drive/My\\ Drive/fsganLab/fshome/weights /content/drive/My\\ Drive/fsganLab\n",
        "!cp /content/drive/My\\ Drive/fsganLab/fshome/fsgan/inference/swap.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tUPOI-MDY1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 注册 fshome\n",
        "!echo 'export PYTHONPATH=$PYTHONPATH:/content/drive/My\\ Drive/fsganLab/fshome' >> ~/.profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gfJfjQxMPVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!source ~/.profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a96GLQqM2NZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 编译安装 opencv (如果要 avc1 编码则需要，无比费时，容易出错)\n",
        "# !wget -O opencv.zip https://github.com/opencv/opencv/archive/4.3.0.zip\n",
        "# !unzip opencv.zip\n",
        "# !rm opencv.zip\n",
        "# %cd /content/opencv-4.3.0\n",
        "# !mkdir build\n",
        "# %cd /content/opencv-4.3.0/build\n",
        "# !cmake -D CMAKE_BUILD_TYPE=RELEASE \\\n",
        "# -D CMAKE_INSTALL_PREFIX=$(python -c \"import sys; print(sys.prefix)\") \\\n",
        "# -D ENABLE_FAST_MATH=ON \\\n",
        "# -D INSTALL_PYTHON_EXAMPLES=OFF \\\n",
        "# -D INSTALL_C_EXAMPLES=OFF \\\n",
        "# -D OPENCV_ENABLE_NONFREE=OFF \\\n",
        "# -D CMAKE_INSTALL_PREFIX=$(python -c \"import sys; print(sys.prefix)\") \\\n",
        "# -D PYTHON_EXECUTABLE=$(which python) \\\n",
        "# -D PYTHON_INCLUDE_DIR=$(python -c \"from distutils.sysconfig import get_python_inc; print(get_python_inc())\") \\\n",
        "# -D PYTHON_PACKAGES_PATH=$(python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\") \\\n",
        "# -D BUILD_EXAMPLES=OFF \\\n",
        "# -D BUILD_JPEG=ON ..\n",
        "# !make -j8\n",
        "# !sudo make install\n",
        "# !sudo ldconfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR5vQfn9s0op",
        "colab_type": "text"
      },
      "source": [
        "# 修改三个文件，防止因为没有编码器报错（你只需要按就 ok）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN1QRFC_szFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/fsganLab/fshome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WwK-FEns97c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 1，使用 mp4v 需要修改 fsgan/utils/video_renderer.py\n",
        "%%writefile fsgan/utils/video_renderer.py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from fsgan.utils.img_utils import tensor2bgr\n",
        "from fsgan.utils.bbox_utils import crop2img, scale_bbox\n",
        "\n",
        "\n",
        "class VideoRenderer(mp.Process):\n",
        "    \"\"\" Renders input video frames to both screen and video file.\n",
        "\n",
        "    For more control on the rendering, this class should be inherited from and the on_render method overridden\n",
        "    with an application specific implementation.\n",
        "\n",
        "    Args:\n",
        "        display (bool): If True, the rendered video will be displayed on screen\n",
        "        verbose (int): Verbose level. Controls the amount of debug information in the rendering\n",
        "        verbose_size (tuple of int): The rendered frame size for verbose level other than zero (width, height)\n",
        "        output_crop (bool): If True, a cropped frame of size (resolution, resolution) will be rendered for\n",
        "            verbose level zero\n",
        "        resolution (int): Determines the size of cropped frames to be (resolution, resolution)\n",
        "        crop_scale (float): Multiplier factor to scale tight bounding boxes\n",
        "    \"\"\"\n",
        "    def __init__(self, display=False, verbose=0, verbose_size=None, output_crop=False, resolution=256, crop_scale=1.2):\n",
        "        super(VideoRenderer, self).__init__()\n",
        "        self._display = display\n",
        "        self._verbose = verbose\n",
        "        self._verbose_size = verbose_size\n",
        "        self._output_crop = output_crop\n",
        "        self._resolution = resolution\n",
        "        self._crop_scale = crop_scale\n",
        "        self._running = True\n",
        "        self._input_queue = mp.Queue()\n",
        "        self._reply_queue = mp.Queue()\n",
        "        self._fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        self._in_vid = None\n",
        "        self._out_vid = None\n",
        "        self._seq = None\n",
        "        self._in_vid_path = None\n",
        "        self._total_frames = None\n",
        "        self._frame_count = 0\n",
        "\n",
        "    def init(self, in_vid_path, seq, out_vid_path=None, **kwargs):\n",
        "        \"\"\" Initialize the video render for a new video rendering job.\n",
        "\n",
        "        Args:\n",
        "            in_vid_path (str): Input video path\n",
        "            seq (Sequence): Input sequence corresponding to the input video\n",
        "            out_vid_path (str, optional): If specified, the rendering will be written to an output video in that path\n",
        "            **kwargs (dict): Additional keyword arguments that will be added as members of the class. This allows\n",
        "                inheriting classes to access those arguments from the new process\n",
        "        \"\"\"\n",
        "        self._input_queue.put([in_vid_path, seq, out_vid_path, kwargs])\n",
        "\n",
        "    def write(self, *args):\n",
        "        \"\"\" Add tensors for rendering.\n",
        "\n",
        "        Args:\n",
        "            *args (tuple of torch.Tensor): The tensors for rendering\n",
        "        \"\"\"\n",
        "        self._input_queue.put([a.cpu() for a in args])\n",
        "\n",
        "    def wait_until_finished(self):\n",
        "        \"\"\" Wait for the video renderer to finish the current video rendering job. \"\"\"\n",
        "        return self._reply_queue.get()\n",
        "\n",
        "    def on_render(self, *args):\n",
        "        \"\"\" Given the input tensors this method produces a cropped rendered image.\n",
        "\n",
        "        This method should be overridden by inheriting classes to customize the rendering. By default this method\n",
        "        expects the first tensor to be a cropped image tensor of shape (B, 3, H, W) where B is the batch size,\n",
        "        H is the height of the image and W is the width of the image.\n",
        "\n",
        "        Args:\n",
        "            *args (tuple of torch.Tensor): The tensors for rendering\n",
        "\n",
        "        Returns:\n",
        "            render_bgr (np.array): The cropped rendered image\n",
        "        \"\"\"\n",
        "        return tensor2bgr(args[0])\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\" Main processing loop. Intended to be executed on a separate process. \"\"\"\n",
        "        while self._running:\n",
        "            task = self._input_queue.get()\n",
        "\n",
        "            # Initialize new video rendering task\n",
        "            if self._in_vid is None:\n",
        "                self._in_vid_path, self._seq, out_vid_path = task[:3]\n",
        "                additional_attributes = task[3]\n",
        "                self._frame_count = 0\n",
        "\n",
        "                # Add additional arguments as members\n",
        "                for attr_name, attr_val in additional_attributes.items():\n",
        "                    setattr(self, attr_name, attr_val)\n",
        "\n",
        "                # Open input video\n",
        "                self._in_vid = cv2.VideoCapture(self._in_vid_path)\n",
        "                assert self._in_vid.isOpened(), f'Failed to open video: \"{self._in_vid_path}\"'\n",
        "\n",
        "                in_total_frames = int(self._in_vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                fps = self._in_vid.get(cv2.CAP_PROP_FPS)\n",
        "                in_vid_width = int(self._in_vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                in_vid_height = int(self._in_vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                self._total_frames = in_total_frames if self._verbose == 0 else len(self._seq)\n",
        "                # print(f'Debug: initializing video: \"{self._in_vid_path}\", total_frames={self._total_frames}')\n",
        "\n",
        "                # Initialize output video\n",
        "                if out_vid_path is not None:\n",
        "                    out_size = (in_vid_width, in_vid_height)\n",
        "                    if self._verbose <= 0 and self._output_crop:\n",
        "                        out_size = (self._resolution, self._resolution)\n",
        "                    elif self._verbose_size is not None:\n",
        "                        out_size = self._verbose_size\n",
        "                    self._out_vid = cv2.VideoWriter(out_vid_path, self._fourcc, fps, out_size)\n",
        "\n",
        "                # Write frames as they are until the start of the sequence\n",
        "                if self._verbose == 0:\n",
        "                    for i in range(self._seq.start_index):\n",
        "                        # Read frame\n",
        "                        ret, frame_bgr = self._in_vid.read()\n",
        "                        assert frame_bgr is not None, f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "                        self._render(frame_bgr)\n",
        "                        self._frame_count += 1\n",
        "\n",
        "                continue\n",
        "\n",
        "            # Write a batch of frames\n",
        "            tensors = task\n",
        "            batch_size = tensors[0].shape[0]\n",
        "\n",
        "            # For each frame in the current batch of tensors\n",
        "            for b in range(batch_size):\n",
        "                # Handle full frames if output_crop was not specified\n",
        "                full_frame_bgr, bbox = None, None\n",
        "                if self._verbose == 0 and not self._output_crop:\n",
        "                    # Read frame from input video\n",
        "                    ret, full_frame_bgr = self._in_vid.read()\n",
        "                    assert full_frame_bgr is not None, \\\n",
        "                        f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "\n",
        "                    # Get bounding box from sequence\n",
        "                    det = self._seq[self._frame_count - self._seq.start_index]\n",
        "                    bbox = np.concatenate((det[:2], det[2:] - det[:2]))\n",
        "                    bbox = scale_bbox(bbox, self._crop_scale)\n",
        "\n",
        "                render_bgr = self.on_render(*[t[b] for t in tensors])\n",
        "                self._render(render_bgr, full_frame_bgr, bbox)\n",
        "                self._frame_count += 1\n",
        "                # print(f'Debug: Writing frame: {self._frame_count}')\n",
        "\n",
        "            # Check if we reached the end of the sequence\n",
        "            if self._verbose == 0 and self._frame_count >= (self._seq.start_index + len(self._seq)):\n",
        "                for i in range(self._seq.start_index + len(self._seq), self._total_frames):\n",
        "                    # Read frame\n",
        "                    ret, frame_bgr = self._in_vid.read()\n",
        "                    assert frame_bgr is not None, f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "                    self._render(frame_bgr)\n",
        "                    self._frame_count += 1\n",
        "\n",
        "            # Check if all frames have been processed\n",
        "            if self._frame_count >= self._total_frames:\n",
        "                # Clean up\n",
        "                self._in_vid.release()\n",
        "                self._out_vid.release()\n",
        "                self._in_vid = None\n",
        "                self._out_vid = None\n",
        "                self._seq = None\n",
        "                self._in_vid_path = None\n",
        "                self._total_frames = None\n",
        "                self._frame_count = 0\n",
        "\n",
        "                # Notify job is finished\n",
        "                self._reply_queue.put(True)\n",
        "\n",
        "    def _render(self, render_bgr, full_frame_bgr=None, bbox=None):\n",
        "        if self._verbose == 0 and not self._output_crop and full_frame_bgr is not None:\n",
        "            render_bgr = crop2img(full_frame_bgr, render_bgr, bbox)\n",
        "        if self._out_vid is not None:\n",
        "            self._out_vid.write(render_bgr)\n",
        "        if self._display:\n",
        "            cv2.imshow('render', render_bgr)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                self._running = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTD3d-1StZjn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 2，使用 mp4v 需要修改 fsgan/preprocess/preprocess_video.py\n",
        "%%writefile fsgan/preprocess/preprocess_video.py\n",
        "\"\"\" Video preprocessing.\n",
        "\n",
        "This script implements all preprocessing required for both training and inference.\n",
        "The preprocessing information will be cached in a directory by the file's name without the extension,\n",
        "residing in the same directory as the file. The information contains: face detections, face sequences,\n",
        "and cropped videos per sequence. In addition for each cropped video, the corresponding pose, landmarks, and\n",
        "segmentation masks will be computed and cached.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from face_detection_dsfd.face_detector import FaceDetector\n",
        "from fsgan.utils.utils import set_device, load_model\n",
        "from fsgan.preprocess.detections2sequences_center import main as detections2sequences_main\n",
        "from fsgan.preprocess.crop_video_sequences import main as crop_video_sequences_main\n",
        "from fsgan.preprocess.crop_image_sequences import main as crop_image_sequences_main\n",
        "from fsgan.datasets.video_inference_dataset import VideoInferenceDataset\n",
        "import fsgan.datasets.img_landmarks_transforms as img_landmarks_transforms\n",
        "from fsgan.datasets.img_landmarks_transforms import Resize, ToTensor\n",
        "from fsgan.utils.temporal_smoothing import TemporalSmoothing\n",
        "from fsgan.utils.landmarks_utils import LandmarksHeatMapEncoder, smooth_landmarks_98pts\n",
        "from fsgan.utils.seg_utils import encode_binary_mask, remove_inner_mouth\n",
        "from fsgan.utils.batch import main as batch\n",
        "\n",
        "\n",
        "base_parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n",
        "\n",
        "general = base_parser.add_argument_group('general')\n",
        "general.add_argument('-r', '--resolution', default=256, type=int, metavar='N',\n",
        "                     help='finest processing resolution')\n",
        "general.add_argument('-cs', '--crop_scale', default=1.2, type=float, metavar='F',\n",
        "                     help='crop scale relative to bounding box')\n",
        "general.add_argument('--gpus', default=None, nargs='+', type=int, metavar='N',\n",
        "                     help='list of gpu ids to use')\n",
        "general.add_argument('--cpu_only', action='store_true',\n",
        "                     help='force cpu only')\n",
        "general.add_argument('-d', '--display', action='store_true',\n",
        "                     help='display the rendering')\n",
        "general.add_argument('-v', '--verbose', default=0, type=int, metavar='N',\n",
        "                     help='verbose level')\n",
        "\n",
        "detection = base_parser.add_argument_group('detection')\n",
        "detection.add_argument('-dm', '--detection_model', metavar='PATH', default='../weights/WIDERFace_DSFD_RES152.pth',\n",
        "                       help='path to face detection model')\n",
        "detection.add_argument('-db', '--det_batch_size', default=8, type=int, metavar='N',\n",
        "                       help='detection batch size')\n",
        "detection.add_argument('-dp', '--det_postfix', default='_dsfd.pkl', metavar='POSTFIX',\n",
        "                       help='detection file postfix')\n",
        "\n",
        "sequences = base_parser.add_argument_group('sequences')\n",
        "sequences.add_argument('-it', '--iou_thresh', default=0.75, type=float,\n",
        "                       metavar='F', help='IOU threshold')\n",
        "sequences.add_argument('-ml', '--min_length', default=10, type=int,\n",
        "                       metavar='N', help='minimum sequence length')\n",
        "sequences.add_argument('-ms', '--min_size', default=64, type=int,\n",
        "                       metavar='N', help='minimum sequence average bounding box size')\n",
        "sequences.add_argument('-ck', '--center_kernel', default=25, type=int,\n",
        "                       metavar='N', help='center average kernel size')\n",
        "sequences.add_argument('-sk', '--size_kernel', default=51, type=int,\n",
        "                       metavar='N', help='size average kernel size')\n",
        "sequences.add_argument('-dsd', '--disable_smooth_det', dest='smooth_det', action='store_false',\n",
        "                       help='disable smoothing the detection bounding boxes')\n",
        "sequences.add_argument('-sp', '--seq_postfix', default='_dsfd_seq.pkl', metavar='POSTFIX',\n",
        "                       help='sequence file postfix')\n",
        "sequences.add_argument('-we', '--write_empty', action='store_true',\n",
        "                       help='write empty sequence lists to file')\n",
        "\n",
        "pose = base_parser.add_argument_group('pose')\n",
        "pose.add_argument('-pm', '--pose_model', default='../weights/hopenet_robust_alpha1.pth', metavar='PATH',\n",
        "                       help='path to face pose model file')\n",
        "pose.add_argument('-pb', '--pose_batch_size', default=128, type=int, metavar='N',\n",
        "                       help='pose batch size')\n",
        "pose.add_argument('-pp', '--pose_postfix', default='_pose.npz', metavar='POSTFIX',\n",
        "                       help='pose file postfix')\n",
        "pose.add_argument('-cp', '--cache_pose', action='store_true',\n",
        "                  help='Toggle whether to cache pose')\n",
        "pose.add_argument('-cf', '--cache_frontal', action='store_true',\n",
        "                  help='Toggle whether to cache frontal images for each sequence')\n",
        "pose.add_argument('-spo', '--smooth_poses', default=5, type=int, metavar='N',\n",
        "                  help='poses temporal smoothing kernel size')\n",
        "\n",
        "landmarks = base_parser.add_argument_group('landmarks')\n",
        "landmarks.add_argument('-lm', '--lms_model', default='../weights/hr18_wflw_landmarks.pth', metavar='PATH',\n",
        "                       help='landmarks model')\n",
        "landmarks.add_argument('-lb', '--lms_batch_size', default=64, type=int, metavar='N',\n",
        "                       help='landmarks batch size')\n",
        "landmarks.add_argument('-lp', '--landmarks_postfix', default='_lms.npz', metavar='POSTFIX',\n",
        "                       help='landmarks file postfix')\n",
        "landmarks.add_argument('-cl', '--cache_landmarks', action='store_true',\n",
        "                       help='Toggle whether to cache landmarks')\n",
        "landmarks.add_argument('-sl', '--smooth_landmarks', default=7, type=int, metavar='N',\n",
        "                       help='landmarks temporal smoothing kernel size')\n",
        "\n",
        "segmentation = base_parser.add_argument_group('segmentation')\n",
        "segmentation.add_argument('-sm', '--seg_model', default='../weights/celeba_unet_256_1_2_segmentation_v2.pth',\n",
        "                          metavar='PATH', help='segmentation model')\n",
        "segmentation.add_argument('-sb', '--seg_batch_size', default=32, type=int, metavar='N',\n",
        "                          help='segmentation batch size')\n",
        "segmentation.add_argument('-sep', '--segmentation_postfix', default='_seg.pkl', metavar='POSTFIX',\n",
        "                          help='segmentation file postfix')\n",
        "segmentation.add_argument('-cse', '--cache_segmentation', action='store_true',\n",
        "                          help='Toggle whether to cache segmentation')\n",
        "segmentation.add_argument('-sse', '--smooth_segmentation', default=5, type=int, metavar='N',\n",
        "                          help='segmentation temporal smoothing kernel size')\n",
        "segmentation.add_argument('-srm', '--seg_remove_mouth', action='store_true',\n",
        "                          help='if true, the inner part of the mouth will be removed from the segmentation')\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "                                 parents=[base_parser])\n",
        "parser.add_argument('input', metavar='VIDEO', nargs='+',\n",
        "                    help='path to input video')\n",
        "parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                    help='output directory')\n",
        "d = parser.get_default\n",
        "\n",
        "\n",
        "class VideoProcessBase(object):\n",
        "    def __init__(self, resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth')):\n",
        "        # General\n",
        "        self.resolution = resolution\n",
        "        self.crop_scale = crop_scale\n",
        "        self.display = display\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Detection\n",
        "        self.face_detector = FaceDetector(det_postfix, detection_model, gpus, det_batch_size, display)\n",
        "        self.det_postfix = det_postfix\n",
        "\n",
        "        # Sequences\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.min_length = min_length\n",
        "        self.min_size = min_size\n",
        "        self.center_kernel = center_kernel\n",
        "        self.size_kernel = size_kernel\n",
        "        self.smooth_det = smooth_det\n",
        "        self.seq_postfix = seq_postfix\n",
        "        self.write_empty = write_empty\n",
        "\n",
        "        # Pose\n",
        "        self.pose_batch_size = pose_batch_size\n",
        "        self.pose_postfix = pose_postfix\n",
        "        self.cache_pose = cache_pose\n",
        "        self.cache_frontal = cache_frontal\n",
        "        self.smooth_poses = smooth_poses\n",
        "\n",
        "        # Landmarks\n",
        "        self.smooth_landmarks = smooth_landmarks\n",
        "        self.landmarks_postfix = landmarks_postfix\n",
        "        self.cache_landmarks = cache_landmarks\n",
        "        self.lms_batch_size = lms_batch_size\n",
        "\n",
        "        # Segmentation\n",
        "        self.smooth_segmentation = smooth_segmentation\n",
        "        self.segmentation_postfix = segmentation_postfix\n",
        "        self.cache_segmentation = cache_segmentation\n",
        "        self.seg_batch_size = seg_batch_size\n",
        "        self.seg_remove_mouth = seg_remove_mouth and cache_landmarks\n",
        "\n",
        "        # Initialize device\n",
        "        torch.set_grad_enabled(False)\n",
        "        self.device, self.gpus = set_device(gpus, not cpu_only)\n",
        "\n",
        "        # Load models\n",
        "        self.face_pose = load_model(pose_model, 'face pose', self.device) if cache_pose else None\n",
        "        self.L = load_model(lms_model, 'face landmarks', self.device) if cache_landmarks else None\n",
        "        self.S = load_model(seg_model, 'face segmentation', self.device) if cache_segmentation else None\n",
        "\n",
        "        # Initialize heatmap encoder\n",
        "        self.heatmap_encoder = LandmarksHeatMapEncoder().to(self.device)\n",
        "\n",
        "        # Initialize normalization tensors\n",
        "        # Note: this is necessary because of the landmarks model\n",
        "        self.img_mean = torch.as_tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)\n",
        "        self.img_std = torch.as_tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)\n",
        "        self.context_mean = torch.as_tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n",
        "        self.context_std = torch.as_tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n",
        "\n",
        "        # Support multiple GPUs\n",
        "        if self.gpus and len(self.gpus) > 1:\n",
        "            self.face_pose = nn.DataParallel(self.face_pose, self.gpus) if self.face_pose is not None else None\n",
        "            self.L = nn.DataParallel(self.L, self.gpus) if self.L is not None else None\n",
        "            self.S = nn.DataParallel(self.S, self.gpus) if self.S is not None else None\n",
        "\n",
        "        # Initialize temportal smoothing\n",
        "        if smooth_segmentation > 0:\n",
        "            self.smooth_seg = TemporalSmoothing(3, smooth_segmentation).to(self.device)\n",
        "        else:\n",
        "            self.smooth_seg = None\n",
        "\n",
        "        # Initialize output videos format\n",
        "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    def process_pose(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_pose:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            Resize(224), ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_pose_path = os.path.splitext(curr_vid_path)[0] + self.pose_postfix\n",
        "\n",
        "            if os.path.isfile(curr_pose_path):\n",
        "                continue\n",
        "            print('=> Computing face poses for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.pose_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            seq_poses = []\n",
        "            for i, frame in enumerate(tqdm(in_vid_loader, unit='batches')):\n",
        "                frame = frame.to(self.device)\n",
        "                poses = self.face_pose(frame).div_(99.)  # Yaw, Pitch, Roll\n",
        "                seq_poses.append(poses.cpu().numpy())\n",
        "            seq_poses = np.concatenate(seq_poses)\n",
        "\n",
        "            # Save landmarks to file\n",
        "            seq_landmarks_smoothed = smooth_poses(seq_poses, self.smooth_poses)\n",
        "            np.savez_compressed(curr_pose_path, poses=seq_poses, poses_smoothed=seq_landmarks_smoothed)\n",
        "\n",
        "    def extract_frontal_images(self, input_path, output_dir, seq_file_path, out_postfix='.jpg', resolution=None):\n",
        "        if not self.cache_frontal:\n",
        "            return\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d.mp4' % seq.id\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_pose_path = os.path.splitext(curr_vid_path)[0] + self.pose_postfix\n",
        "            curr_frontal_path = os.path.splitext(curr_vid_path)[0] + out_postfix\n",
        "\n",
        "            if os.path.isfile(curr_frontal_path):\n",
        "                continue\n",
        "\n",
        "            # Open current video file\n",
        "            vid = cv2.VideoCapture(curr_vid_path)\n",
        "            if not vid.isOpened():\n",
        "                raise RuntimeError('Failed to read video: ' + curr_vid_path)\n",
        "\n",
        "            # Load current sequence poses\n",
        "            curr_poses = np.load(curr_pose_path)['poses_smoothed']\n",
        "\n",
        "            # Read frontal image from video\n",
        "            frontal_index = np.argmin(np.linalg.norm(curr_poses, axis=1))\n",
        "            vid.set(cv2.CAP_PROP_POS_FRAMES, frontal_index)\n",
        "            ret, frontal_bgr = vid.read()\n",
        "\n",
        "            # Resize image\n",
        "            if resolution is not None:\n",
        "                frontal_bgr = cv2.resize(frontal_bgr, (resolution, resolution), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Write frontal image to file\n",
        "            cv2.imwrite(curr_frontal_path, frontal_bgr)\n",
        "\n",
        "    def process_landmarks(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_landmarks:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_lms_path = os.path.splitext(curr_vid_path)[0] + self.landmarks_postfix\n",
        "\n",
        "            if os.path.isfile(curr_lms_path):\n",
        "                continue\n",
        "            print('=> Computing face landmarks for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.lms_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            seq_landmarks = []\n",
        "            for i, frame in enumerate(tqdm(in_vid_loader, unit='batches')):\n",
        "                frame = frame.to(self.device)\n",
        "                H = self.L(frame)\n",
        "                landmarks = self.heatmap_encoder(H)\n",
        "                seq_landmarks.append(landmarks.cpu().numpy())\n",
        "            seq_landmarks = np.concatenate(seq_landmarks)\n",
        "\n",
        "            # Save landmarks to file\n",
        "            seq_landmarks_smoothed = smooth_landmarks_98pts(seq_landmarks, self.smooth_landmarks)\n",
        "            np.savez_compressed(curr_lms_path, landmarks=seq_landmarks, landmarks_smoothed=seq_landmarks_smoothed)\n",
        "\n",
        "    def process_segmentation(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_segmentation:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_seg_path = os.path.splitext(curr_vid_path)[0] + self.segmentation_postfix\n",
        "\n",
        "            if self.seg_remove_mouth:\n",
        "                curr_lms_path = os.path.splitext(curr_vid_path)[0] + self.landmarks_postfix\n",
        "                landmarks = np.load(curr_lms_path)['landmarks_smoothed']\n",
        "                frame_count = 0\n",
        "\n",
        "            if os.path.isfile(curr_seg_path):\n",
        "                continue\n",
        "            print('=> Computing face segmentation for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.seg_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            pbar = tqdm(in_vid_loader, unit='batches')\n",
        "            prev_segmentation = None\n",
        "            r = self.smooth_seg.kernel_radius\n",
        "            encoded_segmentations = []\n",
        "            pad_prev, pad_next = r, r   # This initialization is only relevant if there is a leftover from last batch\n",
        "            for i, frame in enumerate(pbar):\n",
        "                frame = frame.to(self.device)\n",
        "\n",
        "                # Compute segmentation\n",
        "                raw_segmentation = self.S(frame)\n",
        "                segmentation = torch.cat((prev_segmentation, raw_segmentation), dim=0) \\\n",
        "                    if prev_segmentation is not None else raw_segmentation\n",
        "                if segmentation.shape[0] > r:\n",
        "                    pad_prev, pad_next = r if prev_segmentation is None else 0, min(r, self.seg_batch_size - frame.shape[0])\n",
        "                    segmentation = self.smooth_seg(segmentation, pad_prev=pad_prev, pad_next=pad_next)\n",
        "\n",
        "                    # Note: the pad_next value here is only relevant if there is a leftover from last batch\n",
        "                    prev_segmentation = raw_segmentation[-(r * 2 - pad_next):]\n",
        "\n",
        "                mask = segmentation.argmax(1) == 1\n",
        "\n",
        "                # Encode segmentation\n",
        "                for b in range(mask.shape[0]):\n",
        "                    curr_mask = mask[b].cpu().numpy()\n",
        "                    if self.seg_remove_mouth:\n",
        "                        curr_mask = remove_inner_mouth(curr_mask, landmarks[frame_count])\n",
        "                        frame_count += 1\n",
        "                    encoded_segmentations.append(encode_binary_mask(curr_mask))\n",
        "\n",
        "            # Final iteration if we have leftover unsmoothed segmentations from the last batch\n",
        "            if pad_next < r:\n",
        "                # Compute segmentation\n",
        "                segmentation = self.smooth_seg(prev_segmentation, pad_prev=pad_prev, pad_next=r)\n",
        "                mask = segmentation.argmax(1) == 1\n",
        "\n",
        "                # Encode segmentation\n",
        "                for b in range(mask.shape[0]):\n",
        "                    curr_mask = mask[b].cpu().numpy()\n",
        "                    if self.seg_remove_mouth:\n",
        "                        curr_mask = remove_inner_mouth(curr_mask, landmarks[frame_count])\n",
        "                        frame_count += 1\n",
        "                    encoded_segmentations.append(encode_binary_mask(curr_mask))\n",
        "\n",
        "            # Write to file\n",
        "            with open(curr_seg_path, \"wb\") as fp:  # Pickling\n",
        "                pickle.dump(encoded_segmentations, fp)\n",
        "\n",
        "\n",
        "    def cache(self, input_path, output_dir=None):\n",
        "        # Validation\n",
        "        assert os.path.isfile(input_path), 'Input path \"%s\" does not exist' % input_path\n",
        "        assert output_dir is None or os.path.isdir(output_dir), 'Output path \"%s\" must be a directory' % output_dir\n",
        "        is_vid = os.path.splitext(input_path)[1] == '.mp4'\n",
        "\n",
        "        # Set paths\n",
        "        output_dir = os.path.splitext(input_path)[0] if output_dir is None else output_dir\n",
        "        det_file_path = os.path.splitext(input_path)[0] + self.det_postfix\n",
        "        if not os.path.isfile(det_file_path):   # Check if there is a detection file in the same directory as the video\n",
        "            det_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] +\n",
        "                                         self.det_postfix)\n",
        "        seq_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + self.seq_postfix)\n",
        "        first_cropped_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] +\n",
        "                                          '_seq00' + os.path.splitext(input_path)[1])\n",
        "        pose_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + self.pose_postfix)\n",
        "\n",
        "        # Create directory\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "\n",
        "        # Face detection\n",
        "        if not os.path.isfile(det_file_path):\n",
        "            self.face_detector(input_path, det_file_path)\n",
        "\n",
        "        # Detections to sequences\n",
        "        if not os.path.isfile(seq_file_path):\n",
        "            detections2sequences_main(input_path, seq_file_path, det_file_path, self.iou_thresh, self.min_length,\n",
        "                                      self.min_size, self.crop_scale, self.center_kernel, self.size_kernel,\n",
        "                                      self.smooth_det, self.display, self.write_empty)\n",
        "\n",
        "        # Crop video sequences\n",
        "        if not os.path.isfile(first_cropped_path):\n",
        "            if is_vid:\n",
        "                crop_video_sequences_main(input_path, output_dir, seq_file_path, self.seq_postfix, self.resolution,\n",
        "                                          self.crop_scale, select='all', disable_tqdm=False)\n",
        "            else:\n",
        "                crop_image_sequences_main(input_path, output_dir, seq_file_path, self.seq_postfix, '.jpg',\n",
        "                                          self.resolution, self.crop_scale)\n",
        "\n",
        "        # Face poses\n",
        "        # if not os.path.isfile(pose_file_path) and is_vid:\n",
        "        #     self.process_pose(input_path, output_dir, seq_file_path, pose_file_path)\n",
        "        # if is_vid:\n",
        "        self.process_pose(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Extract frontal images\n",
        "        # if self.cache_pose and self.cache_frontal and is_vid:\n",
        "        #     self.extract_frontal_images(input_path, output_dir, pose_file_path)\n",
        "        if self.cache_pose and self.cache_frontal and is_vid:\n",
        "            self.extract_frontal_images(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Cache landmarks\n",
        "        self.process_landmarks(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Cache segmentation\n",
        "        self.process_segmentation(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        return output_dir, seq_file_path, pose_file_path if self.cache_pose and is_vid else None\n",
        "\n",
        "\n",
        "class VideoProcessCallable(VideoProcessBase):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(VideoProcessCallable, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, input_path, output_dir=None):\n",
        "        return self.cache(input_path, output_dir)\n",
        "\n",
        "\n",
        "def smooth_poses(poses, kernel_size=5):\n",
        "    out_poses = poses.copy() if isinstance(poses, np.ndarray) else np.array(poses)\n",
        "\n",
        "    # Prepare smoothing kernel\n",
        "    # w = np.hamming(kernel_size)\n",
        "    w = np.ones(kernel_size)\n",
        "    w /= w.sum()\n",
        "\n",
        "    # Smooth poses\n",
        "    poses_padded = np.pad(out_poses, ((kernel_size // 2, kernel_size // 2), (0, 0)), 'reflect')\n",
        "    for i in range(out_poses.shape[1]):\n",
        "        out_poses[:, i] = np.convolve(w, poses_padded[:, i], mode='valid')\n",
        "\n",
        "    return out_poses\n",
        "\n",
        "\n",
        "def main(input, output=d('output'), resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth')):\n",
        "    video_process = VideoProcessCallable(\n",
        "        resolution, crop_scale, gpus, cpu_only, display, verbose,\n",
        "        detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "        iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "        size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "        pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix, cache_pose=cache_pose,\n",
        "        cache_frontal=cache_frontal, smooth_poses=smooth_poses, lms_model=lms_model, lms_batch_size=lms_batch_size,\n",
        "        landmarks_postfix=landmarks_postfix, cache_landmarks=cache_landmarks, smooth_landmarks=smooth_landmarks,\n",
        "        seg_model=seg_model, seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "        cache_segmentation=cache_segmentation, smooth_segmentation=smooth_segmentation,\n",
        "        seg_remove_mouth=seg_remove_mouth)\n",
        "    if len(input) == 1 and os.path.isfile(input[0]):\n",
        "        video_process.cache(input, output)\n",
        "    else:\n",
        "        batch(input, None, output, video_process, postfix='.mp4')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(**vars(parser.parse_args()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbcZplL8uH1d",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 3，使用 mp4v 需要修改 fsgan/preprocess/crop_image_sequences.py\n",
        "%%writefile fsgan/preprocess/crop_image_sequences.py\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from fsgan.utils.bbox_utils import scale_bbox, crop_img\n",
        "from fsgan.utils.video_utils import Sequence\n",
        "\n",
        "\n",
        "def main(input_path, output_dir=None, cache_path=None, seq_postfix='_dsfd_seq.pkl', resolution=256, crop_scale=2.0,\n",
        "         select='all', disable_tqdm=False):\n",
        "    cache_path = os.path.splitext(input_path)[0] + seq_postfix if cache_path is None else cache_path\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.splitext(input_path)[0]\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "\n",
        "    # Verification\n",
        "    if not os.path.isfile(input_path):\n",
        "        raise RuntimeError('Input video does not exist: ' + input_path)\n",
        "    if not os.path.isfile(cache_path):\n",
        "        raise RuntimeError('Cache file does not exist: ' + cache_path)\n",
        "    if not os.path.isdir(output_dir):\n",
        "        raise RuntimeError('Output directory does not exist: ' + output_dir)\n",
        "\n",
        "    print('=> Cropping video sequences from video: \"%s\"...' % os.path.basename(input_path))\n",
        "\n",
        "    # Load sequences from file\n",
        "    with open(cache_path, \"rb\") as fp:  # Unpickling\n",
        "        seq_list = pickle.load(fp)\n",
        "\n",
        "    # Select sequences\n",
        "    if select == 'longest':\n",
        "        selected_seq_index = np.argmax([len(s) for s in seq_list])\n",
        "        seq = seq_list[selected_seq_index]\n",
        "        seq.id = 0\n",
        "        seq_list = [seq]\n",
        "\n",
        "    # Open input video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError('Failed to read video: ' + input_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    input_vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    input_vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # For each sequence initialize output video file\n",
        "    out_vids = []\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    for seq in seq_list:\n",
        "        curr_vid_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d.mp4' % seq.id\n",
        "        curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "        out_vids.append(cv2.VideoWriter(curr_vid_path, fourcc, fps, (resolution, resolution)))\n",
        "\n",
        "    # For each frame in the target video\n",
        "    cropped_detections = [[] for seq in seq_list]\n",
        "    cropped_landmarks = [[] for seq in seq_list]\n",
        "    pbar = range(total_frames) if disable_tqdm else tqdm(range(total_frames))\n",
        "    for i in pbar:\n",
        "        ret, frame = cap.read()\n",
        "        if frame is None:\n",
        "            continue\n",
        "\n",
        "        # For each sequence\n",
        "        for s, seq in enumerate(seq_list):\n",
        "            if i < seq.start_index or (seq.start_index + len(seq) - 1) < i:\n",
        "                continue\n",
        "            det = seq[i - seq.start_index]\n",
        "\n",
        "            # Crop frame\n",
        "            bbox = np.concatenate((det[:2], det[2:] - det[:2]))\n",
        "            bbox = scale_bbox(bbox, crop_scale)\n",
        "            frame_cropped = crop_img(frame, bbox)\n",
        "            frame_cropped = cv2.resize(frame_cropped, (resolution, resolution), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Write cropped frame to output video\n",
        "            out_vids[s].write(frame_cropped)\n",
        "\n",
        "            # Add cropped detection to list\n",
        "            orig_size = bbox[2:]\n",
        "            axes_scale = np.array([resolution, resolution]) / orig_size\n",
        "            det[:2] -= bbox[:2]\n",
        "            det[2:] -= bbox[:2]\n",
        "            det[:2] *= axes_scale\n",
        "            det[2:] *= axes_scale\n",
        "            cropped_detections[s].append(det)\n",
        "\n",
        "            # Add cropped landmarks to list\n",
        "            if hasattr(seq, 'landmarks'):\n",
        "                curr_landmarks = seq.landmarks[i - seq.start_index]\n",
        "                curr_landmarks[:, :2] -= bbox[:2]\n",
        "\n",
        "                # 3D landmarks case\n",
        "                if curr_landmarks.shape[1] == 3:\n",
        "                    axes_scale = np.append(axes_scale, axes_scale.mean())\n",
        "\n",
        "                curr_landmarks *= axes_scale\n",
        "                cropped_landmarks[s].append(curr_landmarks)\n",
        "\n",
        "    # For each sequence write cropped sequence to file\n",
        "    for s, seq in enumerate(seq_list):\n",
        "        # seq.detections = np.array(cropped_detections[s])\n",
        "        # if hasattr(seq, 'landmarks'):\n",
        "        #     seq.landmarks = np.array(cropped_landmarks[s])\n",
        "        # seq.start_index = 0\n",
        "\n",
        "        # TODO: this is a hack to change class type (remove this later)\n",
        "        out_seq = Sequence(0)\n",
        "        out_seq.detections = np.array(cropped_detections[s])\n",
        "        if hasattr(seq, 'landmarks'):\n",
        "            out_seq.landmarks = np.array(cropped_landmarks[s])\n",
        "        out_seq.id, out_seq.obj_id, out_seq.size_avg = seq.id, seq.obj_id, seq.size_avg\n",
        "\n",
        "        # Write to file\n",
        "        curr_out_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d%s' % (out_seq.id, seq_postfix)\n",
        "        curr_out_path = os.path.join(output_dir, curr_out_name)\n",
        "        with open(curr_out_path, \"wb\") as fp:  # Pickling\n",
        "            pickle.dump([out_seq], fp)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parse program arguments\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser('crop_video_sequences')\n",
        "    parser.add_argument('input', metavar='VIDEO',\n",
        "                        help='path to input video')\n",
        "    parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                        help='output directory')\n",
        "    parser.add_argument('-c', '--cache', metavar='PATH',\n",
        "                        help='path to sequence cache file')\n",
        "    parser.add_argument('-sp', '--seq_postfix', default='_dsfd_seq.pkl', metavar='POSTFIX',\n",
        "                        help='input sequence file postfix')\n",
        "    parser.add_argument('-r', '--resolution', default=256, type=int, metavar='N',\n",
        "                        help='output video resolution (default: 256)')\n",
        "    parser.add_argument('-cs', '--crop_scale', default=2.0, type=float, metavar='F',\n",
        "                        help='crop scale relative to bounding box (default: 2.0)')\n",
        "    parser.add_argument('-s', '--select', default='all', metavar='STR',\n",
        "                        help='selection method [all|longest]')\n",
        "    parser.add_argument('-dt', '--disable_tqdm', dest='disable_tqdm', action='store_true',\n",
        "                          help='if specified disables tqdm progress bar')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input, args.output, args.cache, args.seq_postfix, args.resolution, args.crop_scale, args.select,\n",
        "         args.disable_tqdm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEeE1IsG2Sw",
        "colab_type": "text"
      },
      "source": [
        "# 使用\n",
        "\n",
        "python swap.py SOURCE -t TARGET -o OUTPUT [FLAGS]\n",
        "\n",
        "SOURCE 为源 mp4\n",
        "\n",
        "TARGET 为目标 mp4\n",
        "\n",
        "OUTPUT 输出路径\n",
        "\n",
        "[FLAGS] 各类参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJN2z8S64m1b",
        "colab_type": "code",
        "outputId": "b1dd14f7-1e39-42b8-8275-a6c5c002c292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "# 1，生成最佳质量的视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py '/content/drive/My Drive/fsganLab/fshome/fsgan/docs/examples/shinzo_abe.mp4' -t '/content/drive/My Drive/fsganLab/fshome/fsgan/docs/examples/girl-B.mp4' -o . --finetune --finetune_save --seg_remove_mouth"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fsganLab/fshome\n",
            "=> using GPU devices: 0\n",
            "=> Loading face pose model: \"hopenet_robust_alpha1.pth\"...\n",
            "=> Loading face landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
            "=> Loading face segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
            "=> Loading face reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
            "=> Loading face completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
            "=> Loading face blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n",
            "=> Detecting faces in video: \"girl-B.mp4...\"\n",
            "100% 207/207 [00:39<00:00,  5.19frames/s]\n",
            "=> Extracting sequences from detections in video: \"girl-B.mp4\"...\n",
            "100% 208/208 [00:00<00:00, 14431.06it/s]\n",
            "=> Cropping video sequences from video: \"girl-B.mp4\"...\n",
            "100% 207/207 [00:01<00:00, 185.43it/s]\n",
            "=> Computing face poses for video: \"girl-B_seq00.mp4\"...\n",
            "100% 2/2 [00:01<00:00,  1.68batches/s]\n",
            "=> Computing face landmarks for video: \"girl-B_seq00.mp4\"...\n",
            "100% 4/4 [00:01<00:00,  2.86batches/s]\n",
            "=> Computing face segmentation for video: \"girl-B_seq00.mp4\"...\n",
            "100% 7/7 [00:02<00:00,  2.93batches/s]\n",
            "=> Loading the reenactment generator finetuned on: \"shinzo_abe_seq00.mp4\"...\n",
            "=> Face swapping: \"shinzo_abe_seq00.mp4\" -> \"girl-B_seq00.mp4\"...\n",
            "100% 26/26 [00:38<00:00,  1.49s/batches]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cntN0ZGir5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2，生成最佳效率的视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --seg_remove_mouth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4WqLJpvYgpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3，生成头部转后直方视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --output_crop -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YNXfFs8bsBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4，生成比较用的直方视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --verbose 1 -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHg5JTwib-Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5，输出所有生产信息（包括过程，主要用于调试）\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --verbose 2 -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvO76eTjSwdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}