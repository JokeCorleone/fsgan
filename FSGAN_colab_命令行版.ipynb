{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FSGAN（conda版V2020.4.25）.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1SZW9d7yu4Al",
        "XR5vQfn9s0op",
        "7wEeE1IsG2Sw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjandnn/fsgan/blob/master/FSGAN%EF%BC%88conda%E7%89%88V2020_4_25%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE0fIrpau-nv",
        "colab_type": "text"
      },
      "source": [
        "# **FSGAN**\n",
        "作者：[Yuval Nirkin](https://github.com/YuvalNirkin)\n",
        "\n",
        "colab： jjandnn ， zhuhaozh ， wangchao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZW9d7yu4Al",
        "colab_type": "text"
      },
      "source": [
        "# 安装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95qjm6RkFX1i",
        "colab_type": "code",
        "outputId": "564cd5e4-531b-4bbf-ed25-9b6bce4dccc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# 必须 p100 \n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 25 11:08:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orFKdoubPfeX",
        "colab_type": "code",
        "outputId": "fb77078e-28e7-4eff-b474-4d9b3ec8f72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!rm -rf /content/sample_data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1L4H_tvXX-",
        "colab_type": "code",
        "outputId": "8237f6cd-85e5-40df-ce88-7792129ccbb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local\n",
        "!rm Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-25 11:24:37--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  37%[======>             ]  30.71M   153MB/s               \r        Miniconda3-  95%[==================> ]  77.20M   193MB/s               \rMiniconda3-latest-L 100%[===================>]  81.12M   195MB/s    in 0.4s    \n",
            "\n",
            "2020-04-25 11:24:38 (195 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhQAt7jCxj_f",
        "colab_type": "code",
        "outputId": "437d4fb1-b599-4569-a89b-bd96a15d9457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 依赖，请勿修改安装顺序\n",
        "!sudo apt-get install build-essential cmake unzip pkg-config\n",
        "!sudo apt-get install libjpeg-dev libpng-dev \n",
        "!sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev \n",
        "!sudo apt-get install libxvidcore-dev libx264-dev\n",
        "!sudo apt-get install libatlas-base-dev gfortran\n",
        "!sudo apt-get install python3-dev\n",
        "!conda install numpy -y\n",
        "\n",
        "!sudo add-apt-repository ppa:jonathonf/ffmpeg-4\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install ffmpeg\n",
        "\n",
        "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch -y\n",
        "!conda install -c anaconda ipykernel -y\n",
        "!conda install -c conda-forge yacs -y\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install tensorboardX\n",
        "!pip install ffmpeg-python\n",
        "!pip install face_alignment"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "libpng-dev is already the newest version (1.6.34-1ubuntu0.18.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libavcodec-dev is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "libavcodec-dev set to manually installed.\n",
            "libavformat-dev is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "libavformat-dev set to manually installed.\n",
            "libswscale-dev is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "libswscale-dev set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  libv4l-0 libv4l-dev libv4l2rds0 libv4lconvert0\n",
            "0 upgraded, 4 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 241 kB of archives.\n",
            "After this operation, 1,028 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libv4lconvert0 amd64 1.14.2-1 [76.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libv4l-0 amd64 1.14.2-1 [41.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libv4l2rds0 amd64 1.14.2-1 [15.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libv4l-dev amd64 1.14.2-1 [107 kB]\n",
            "Fetched 241 kB in 1s (370 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libv4lconvert0:amd64.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../libv4lconvert0_1.14.2-1_amd64.deb ...\n",
            "Unpacking libv4lconvert0:amd64 (1.14.2-1) ...\n",
            "Selecting previously unselected package libv4l-0:amd64.\n",
            "Preparing to unpack .../libv4l-0_1.14.2-1_amd64.deb ...\n",
            "Unpacking libv4l-0:amd64 (1.14.2-1) ...\n",
            "Selecting previously unselected package libv4l2rds0:amd64.\n",
            "Preparing to unpack .../libv4l2rds0_1.14.2-1_amd64.deb ...\n",
            "Unpacking libv4l2rds0:amd64 (1.14.2-1) ...\n",
            "Selecting previously unselected package libv4l-dev:amd64.\n",
            "Preparing to unpack .../libv4l-dev_1.14.2-1_amd64.deb ...\n",
            "Unpacking libv4l-dev:amd64 (1.14.2-1) ...\n",
            "Setting up libv4l2rds0:amd64 (1.14.2-1) ...\n",
            "Setting up libv4lconvert0:amd64 (1.14.2-1) ...\n",
            "Setting up libv4l-0:amd64 (1.14.2-1) ...\n",
            "Setting up libv4l-dev:amd64 (1.14.2-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libx264-dev libxvidcore-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 688 kB of archives.\n",
            "After this operation, 2,851 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libx264-dev amd64 2:0.152.2854+gite9a5903-2 [461 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libxvidcore-dev amd64 2:1.3.5-1 [226 kB]\n",
            "Fetched 688 kB in 1s (921 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libx264-dev:amd64.\n",
            "(Reading database ... 144618 files and directories currently installed.)\n",
            "Preparing to unpack .../libx264-dev_2%3a0.152.2854+gite9a5903-2_amd64.deb ...\n",
            "Unpacking libx264-dev:amd64 (2:0.152.2854+gite9a5903-2) ...\n",
            "Selecting previously unselected package libxvidcore-dev:amd64.\n",
            "Preparing to unpack .../libxvidcore-dev_2%3a1.3.5-1_amd64.deb ...\n",
            "Unpacking libxvidcore-dev:amd64 (2:1.3.5-1) ...\n",
            "Setting up libx264-dev:amd64 (2:0.152.2854+gite9a5903-2) ...\n",
            "Setting up libxvidcore-dev:amd64 (2:1.3.5-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libatlas-base-dev is already the newest version (3.10.3-5).\n",
            "gfortran is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - numpy\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    certifi-2020.4.5.1         |           py37_0         155 KB\n",
            "    conda-4.8.3                |           py37_0         2.8 MB\n",
            "    intel-openmp-2020.0        |              166         756 KB\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_0        1006 KB\n",
            "    mkl-2020.0                 |              166       128.9 MB\n",
            "    mkl-service-2.3.0          |   py37he904b0f_0         218 KB\n",
            "    mkl_fft-1.0.15             |   py37ha843d7b_0         154 KB\n",
            "    mkl_random-1.1.0           |   py37hd6b4f25_0         321 KB\n",
            "    numpy-1.18.1               |   py37h4f9e942_0           5 KB\n",
            "    numpy-base-1.18.1          |   py37hde5b4d6_1         4.2 MB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         2.5 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       141.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2020.0-166\n",
            "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.3.0-hdf63c60_0\n",
            "  mkl                pkgs/main/linux-64::mkl-2020.0-166\n",
            "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py37he904b0f_0\n",
            "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.0.15-py37ha843d7b_0\n",
            "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.0-py37hd6b4f25_0\n",
            "  numpy              pkgs/main/linux-64::numpy-1.18.1-py37h4f9e942_0\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.18.1-py37hde5b4d6_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  certifi                                 2019.11.28-py37_0 --> 2020.4.5.1-py37_0\n",
            "  conda                                        4.8.2-py37_0 --> 4.8.3-py37_0\n",
            "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1g-h7b6447c_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "mkl_fft-1.0.15       | 154 KB    | : 100% 1.0/1 [00:00<00:00,  7.03it/s]                \n",
            "certifi-2020.4.5.1   | 155 KB    | : 100% 1.0/1 [00:00<00:00, 17.19it/s]\n",
            "openssl-1.1.1g       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  7.90it/s]\n",
            "mkl-service-2.3.0    | 218 KB    | : 100% 1.0/1 [00:00<00:00, 22.14it/s]\n",
            "mkl_random-1.1.0     | 321 KB    | : 100% 1.0/1 [00:00<00:00, 22.09it/s]\n",
            "blas-1.0             | 6 KB      | : 100% 1.0/1 [00:00<00:00, 25.44it/s]\n",
            "numpy-base-1.18.1    | 4.2 MB    | : 100% 1.0/1 [00:00<00:00,  5.04it/s]\n",
            "numpy-1.18.1         | 5 KB      | : 100% 1.0/1 [00:00<00:00, 24.74it/s]\n",
            "mkl-2020.0           | 128.9 MB  | : 100% 1.0/1 [00:04<00:00,  4.09s/it]               \n",
            "intel-openmp-2020.0  | 756 KB    | : 100% 1.0/1 [00:00<00:00, 15.16it/s]\n",
            "conda-4.8.3          | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  4.94it/s]\n",
            "libgfortran-ng-7.3.0 | 1006 KB   | : 100% 1.0/1 [00:00<00:00, 12.72it/s]\n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            " Backport of FFmpeg 4 and associated libraries. Now includes AOM/AV1 support!\n",
            "\n",
            "FDK AAC is not compatible with GPL and FFmpeg can't be redistributed with it included. Please don't ask for it to be added to this public PPA.\n",
            "\n",
            "---\n",
            "\n",
            "PPA supporters:\n",
            "\n",
            "BigBlueButton (https://bigbluebutton.org)\n",
            "\n",
            "---\n",
            "\n",
            "Donate to FFMPEG: https://ffmpeg.org/donations.html\n",
            "Donate to Debian: https://www.debian.org/donations\n",
            "Donate to this PPA: https://ko-fi.com/jonathonf\n",
            " More info: https://launchpad.net/~jonathonf/+archive/ubuntu/ffmpeg-4\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n",
            "\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [88.1 kB]\n",
            "Get:12 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [889 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,184 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.4 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [839 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [44.6 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,213 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [12.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,372 kB]\n",
            "Get:24 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 Packages [10.2 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [59.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [8,286 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [7,671 B]\n",
            "Get:28 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,811 kB]\n",
            "Get:29 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [874 kB]\n",
            "Fetched 7,554 kB in 3s (2,394 kB/s)\n",
            "Reading package lists... Done\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaom0 libavcodec58 libavdevice58 libavfilter7 libavformat58 libavresample4\n",
            "  libavutil56 libcodec2-0.7 liblilv-0-0 libmysofa1 libpostproc55 libserd-0-0\n",
            "  libsord-0-0 libsratom-0-0 libswresample3 libswscale5 libvidstab1.1\n",
            "  libx264-155 libx265-188\n",
            "Suggested packages:\n",
            "  ffmpeg-doc serdi sordi\n",
            "The following NEW packages will be installed:\n",
            "  libaom0 libavcodec58 libavdevice58 libavfilter7 libavformat58 libavresample4\n",
            "  libavutil56 libcodec2-0.7 liblilv-0-0 libmysofa1 libpostproc55 libserd-0-0\n",
            "  libsord-0-0 libsratom-0-0 libswresample3 libswscale5 libvidstab1.1\n",
            "  libx264-155 libx265-188\n",
            "The following packages will be upgraded:\n",
            "  ffmpeg\n",
            "1 upgraded, 19 newly installed, 0 to remove and 100 not upgraded.\n",
            "Need to get 12.4 MB of archives.\n",
            "After this operation, 46.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcodec2-0.7 amd64 0.7-1 [202 kB]\n",
            "Get:2 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libaom0 amd64 1.0.0.errata1-3~18.04.york0 [1,165 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserd-0-0 amd64 0.28.0~dfsg0-1 [37.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsord-0-0 amd64 0.16.0~dfsg0-1 [20.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsratom-0-0 amd64 0.6.0~dfsg0-1 [15.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblilv-0-0 amd64 0.24.2~dfsg0-1 [38.0 kB]\n",
            "Get:7 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavutil56 amd64 7:4.2.2-1ubuntu1~18.04.york0 [251 kB]\n",
            "Get:8 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libswresample3 amd64 7:4.2.2-1ubuntu1~18.04.york0 [69.4 kB]\n",
            "Get:9 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libx264-155 amd64 2:0.155.2917+git0a84d98-2~18.04.york0 [529 kB]\n",
            "Get:10 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libx265-188 amd64 3.3-0york1~18.04 [1,076 kB]\n",
            "Get:11 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavcodec58 amd64 7:4.2.2-1ubuntu1~18.04.york0 [4,917 kB]\n",
            "Get:12 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavformat58 amd64 7:4.2.2-1ubuntu1~18.04.york0 [1,016 kB]\n",
            "Get:13 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libmysofa1 amd64 1.0~dfsg0-2~18.04.york0 [39.3 kB]\n",
            "Get:14 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libpostproc55 amd64 7:4.2.2-1ubuntu1~18.04.york0 [64.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libswscale5 amd64 7:4.2.2-1ubuntu1~18.04.york0 [168 kB]\n",
            "Get:16 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libvidstab1.1 amd64 1.1.0-2~18.04.york1 [36.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavfilter7 amd64 7:4.2.2-1ubuntu1~18.04.york0 [1,092 kB]\n",
            "Get:18 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavdevice58 amd64 7:4.2.2-1ubuntu1~18.04.york0 [89.1 kB]\n",
            "Get:19 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 libavresample4 amd64 7:4.2.2-1ubuntu1~18.04.york0 [66.8 kB]\n",
            "Get:20 http://ppa.launchpad.net/jonathonf/ffmpeg-4/ubuntu bionic/main amd64 ffmpeg amd64 7:4.2.2-1ubuntu1~18.04.york0 [1,464 kB]\n",
            "Fetched 12.4 MB in 10s (1,287 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 20.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libaom0:amd64.\n",
            "(Reading database ... 144632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libaom0_1.0.0.errata1-3~18.04.york0_amd64.deb ...\n",
            "Unpacking libaom0:amd64 (1.0.0.errata1-3~18.04.york0) ...\n",
            "Selecting previously unselected package libavutil56:amd64.\n",
            "Preparing to unpack .../01-libavutil56_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavutil56:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libcodec2-0.7:amd64.\n",
            "Preparing to unpack .../02-libcodec2-0.7_0.7-1_amd64.deb ...\n",
            "Unpacking libcodec2-0.7:amd64 (0.7-1) ...\n",
            "Selecting previously unselected package libswresample3:amd64.\n",
            "Preparing to unpack .../03-libswresample3_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libswresample3:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libx264-155:amd64.\n",
            "Preparing to unpack .../04-libx264-155_2%3a0.155.2917+git0a84d98-2~18.04.york0_amd64.deb ...\n",
            "Unpacking libx264-155:amd64 (2:0.155.2917+git0a84d98-2~18.04.york0) ...\n",
            "Selecting previously unselected package libx265-188:amd64.\n",
            "Preparing to unpack .../05-libx265-188_3.3-0york1~18.04_amd64.deb ...\n",
            "Unpacking libx265-188:amd64 (3.3-0york1~18.04) ...\n",
            "Selecting previously unselected package libavcodec58:amd64.\n",
            "Preparing to unpack .../06-libavcodec58_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavcodec58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libavformat58:amd64.\n",
            "Preparing to unpack .../07-libavformat58_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavformat58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libserd-0-0:amd64.\n",
            "Preparing to unpack .../08-libserd-0-0_0.28.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libserd-0-0:amd64 (0.28.0~dfsg0-1) ...\n",
            "Selecting previously unselected package libsord-0-0:amd64.\n",
            "Preparing to unpack .../09-libsord-0-0_0.16.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libsord-0-0:amd64 (0.16.0~dfsg0-1) ...\n",
            "Selecting previously unselected package libsratom-0-0:amd64.\n",
            "Preparing to unpack .../10-libsratom-0-0_0.6.0~dfsg0-1_amd64.deb ...\n",
            "Unpacking libsratom-0-0:amd64 (0.6.0~dfsg0-1) ...\n",
            "Selecting previously unselected package liblilv-0-0.\n",
            "Preparing to unpack .../11-liblilv-0-0_0.24.2~dfsg0-1_amd64.deb ...\n",
            "Unpacking liblilv-0-0 (0.24.2~dfsg0-1) ...\n",
            "Selecting previously unselected package libmysofa1:amd64.\n",
            "Preparing to unpack .../12-libmysofa1_1.0~dfsg0-2~18.04.york0_amd64.deb ...\n",
            "Unpacking libmysofa1:amd64 (1.0~dfsg0-2~18.04.york0) ...\n",
            "Selecting previously unselected package libpostproc55:amd64.\n",
            "Preparing to unpack .../13-libpostproc55_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libpostproc55:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libswscale5:amd64.\n",
            "Preparing to unpack .../14-libswscale5_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libswscale5:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libvidstab1.1:amd64.\n",
            "Preparing to unpack .../15-libvidstab1.1_1.1.0-2~18.04.york1_amd64.deb ...\n",
            "Unpacking libvidstab1.1:amd64 (1.1.0-2~18.04.york1) ...\n",
            "Selecting previously unselected package libavfilter7:amd64.\n",
            "Preparing to unpack .../16-libavfilter7_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavfilter7:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libavdevice58:amd64.\n",
            "Preparing to unpack .../17-libavdevice58_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavdevice58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Selecting previously unselected package libavresample4:amd64.\n",
            "Preparing to unpack .../18-libavresample4_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking libavresample4:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Preparing to unpack .../19-ffmpeg_7%3a4.2.2-1ubuntu1~18.04.york0_amd64.deb ...\n",
            "Unpacking ffmpeg (7:4.2.2-1ubuntu1~18.04.york0) over (7:3.4.6-0ubuntu0.18.04.1) ...\n",
            "Setting up libx264-155:amd64 (2:0.155.2917+git0a84d98-2~18.04.york0) ...\n",
            "Setting up libavutil56:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libpostproc55:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libavresample4:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libmysofa1:amd64 (1.0~dfsg0-2~18.04.york0) ...\n",
            "Setting up libx265-188:amd64 (3.3-0york1~18.04) ...\n",
            "Setting up libvidstab1.1:amd64 (1.1.0-2~18.04.york1) ...\n",
            "Setting up libcodec2-0.7:amd64 (0.7-1) ...\n",
            "Setting up libaom0:amd64 (1.0.0.errata1-3~18.04.york0) ...\n",
            "Setting up libserd-0-0:amd64 (0.28.0~dfsg0-1) ...\n",
            "Setting up libswscale5:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libswresample3:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libsord-0-0:amd64 (0.16.0~dfsg0-1) ...\n",
            "Setting up libsratom-0-0:amd64 (0.6.0~dfsg0-1) ...\n",
            "Setting up libavcodec58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up liblilv-0-0 (0.24.2~dfsg0-1) ...\n",
            "Setting up libavformat58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libavfilter7:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up libavdevice58:amd64 (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Setting up ffmpeg (7:4.2.2-1ubuntu1~18.04.york0) ...\n",
            "Removing obsolete conffile /etc/ffserver.conf ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=10.1\n",
            "    - pytorch\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cudatoolkit-10.1.243       |       h6bb024c_0       347.4 MB\n",
            "    freetype-2.9.1             |       h8a8886c_1         550 KB\n",
            "    jpeg-9b                    |       h024ee3a_2         214 KB\n",
            "    libpng-1.6.37              |       hbc83047_0         278 KB\n",
            "    libtiff-4.1.0              |       h2733197_0         447 KB\n",
            "    ninja-1.9.0                |   py37hfd86e86_0         1.2 MB\n",
            "    olefile-0.46               |           py37_0          50 KB\n",
            "    pillow-7.0.0               |   py37hb39fc2d_0         598 KB\n",
            "    pytorch-1.5.0              |py3.7_cuda10.1.243_cudnn7.6.3_0       399.5 MB  pytorch\n",
            "    torchvision-0.6.0          |       py37_cu101        11.8 MB  pytorch\n",
            "    zstd-1.3.7                 |       h0b5b093_0         401 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       762.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0\n",
            "  freetype           pkgs/main/linux-64::freetype-2.9.1-h8a8886c_1\n",
            "  jpeg               pkgs/main/linux-64::jpeg-9b-h024ee3a_2\n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0\n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.1.0-h2733197_0\n",
            "  ninja              pkgs/main/linux-64::ninja-1.9.0-py37hfd86e86_0\n",
            "  olefile            pkgs/main/linux-64::olefile-0.46-py37_0\n",
            "  pillow             pkgs/main/linux-64::pillow-7.0.0-py37hb39fc2d_0\n",
            "  pytorch            pytorch/linux-64::pytorch-1.5.0-py3.7_cuda10.1.243_cudnn7.6.3_0\n",
            "  torchvision        pytorch/linux-64::torchvision-0.6.0-py37_cu101\n",
            "  zstd               pkgs/main/linux-64::zstd-1.3.7-h0b5b093_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "jpeg-9b              | 214 KB    | : 100% 1.0/1 [00:00<00:00,  4.16it/s]                \n",
            "freetype-2.9.1       | 550 KB    | : 100% 1.0/1 [00:00<00:00, 13.80it/s]\n",
            "cudatoolkit-10.1.243 | 347.4 MB  | : 100% 1.0/1 [00:09<00:00,  9.39s/it]               \n",
            "libpng-1.6.37        | 278 KB    | : 100% 1.0/1 [00:00<00:00,  5.02it/s]\n",
            "ninja-1.9.0          | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 13.17it/s]\n",
            "pillow-7.0.0         | 598 KB    | : 100% 1.0/1 [00:06<00:00,  6.84s/it]\n",
            "zstd-1.3.7           | 401 KB    | : 100% 1.0/1 [00:00<00:00, 15.12it/s]\n",
            "olefile-0.46         | 50 KB     | : 100% 1.0/1 [00:00<00:00, 20.91it/s]\n",
            "torchvision-0.6.0    | 11.8 MB   | : 100% 1.0/1 [00:01<00:00, 120.51s/it]               \n",
            "pytorch-1.5.0        | 399.5 MB  | : 100% 1.0/1 [00:53<00:00, 53.15s/it]             \n",
            "libtiff-4.1.0        | 447 KB    | : 100% 1.0/1 [00:00<00:00, 15.06it/s]\n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.1.0             |           py37_0          20 KB  anaconda\n",
            "    ca-certificates-2020.1.1   |                0         132 KB  anaconda\n",
            "    certifi-2020.4.5.1         |           py37_0         159 KB  anaconda\n",
            "    conda-4.8.3                |           py37_0         3.0 MB  anaconda\n",
            "    decorator-4.4.2            |             py_0          14 KB  anaconda\n",
            "    entrypoints-0.3            |           py37_0          12 KB  anaconda\n",
            "    ipykernel-5.1.4            |   py37h39e3cac_0         168 KB  anaconda\n",
            "    ipython-7.13.0             |   py37h5ca1d4c_0         1.1 MB  anaconda\n",
            "    ipython_genutils-0.2.0     |           py37_0          39 KB  anaconda\n",
            "    jedi-0.16.0                |           py37_1         769 KB  anaconda\n",
            "    jupyter_client-6.1.2       |             py_0          81 KB  anaconda\n",
            "    jupyter_core-4.6.3         |           py37_0          75 KB  anaconda\n",
            "    libsodium-1.0.16           |       h1bed415_0         302 KB  anaconda\n",
            "    openssl-1.1.1g             |       h7b6447c_0         3.8 MB  anaconda\n",
            "    parso-0.6.2                |             py_0          69 KB  anaconda\n",
            "    pexpect-4.8.0              |           py37_0          84 KB  anaconda\n",
            "    pickleshare-0.7.5          |           py37_0          13 KB  anaconda\n",
            "    prompt-toolkit-3.0.4       |             py_0         240 KB  anaconda\n",
            "    prompt_toolkit-3.0.4       |                0          11 KB  anaconda\n",
            "    ptyprocess-0.6.0           |           py37_0          23 KB  anaconda\n",
            "    pygments-2.6.1             |             py_0         687 KB  anaconda\n",
            "    python-dateutil-2.8.1      |             py_0         224 KB  anaconda\n",
            "    pyzmq-18.1.1               |   py37he6710b0_0         522 KB  anaconda\n",
            "    tornado-6.0.4              |   py37h7b6447c_1         649 KB  anaconda\n",
            "    traitlets-4.3.3            |           py37_0         138 KB  anaconda\n",
            "    wcwidth-0.1.9              |             py_0          24 KB  anaconda\n",
            "    zeromq-4.3.1               |       he6710b0_3         666 KB  anaconda\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        12.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           anaconda/linux-64::backcall-0.1.0-py37_0\n",
            "  decorator          anaconda/noarch::decorator-4.4.2-py_0\n",
            "  entrypoints        anaconda/linux-64::entrypoints-0.3-py37_0\n",
            "  ipykernel          anaconda/linux-64::ipykernel-5.1.4-py37h39e3cac_0\n",
            "  ipython            anaconda/linux-64::ipython-7.13.0-py37h5ca1d4c_0\n",
            "  ipython_genutils   anaconda/linux-64::ipython_genutils-0.2.0-py37_0\n",
            "  jedi               anaconda/linux-64::jedi-0.16.0-py37_1\n",
            "  jupyter_client     anaconda/noarch::jupyter_client-6.1.2-py_0\n",
            "  jupyter_core       anaconda/linux-64::jupyter_core-4.6.3-py37_0\n",
            "  libsodium          anaconda/linux-64::libsodium-1.0.16-h1bed415_0\n",
            "  parso              anaconda/noarch::parso-0.6.2-py_0\n",
            "  pexpect            anaconda/linux-64::pexpect-4.8.0-py37_0\n",
            "  pickleshare        anaconda/linux-64::pickleshare-0.7.5-py37_0\n",
            "  prompt-toolkit     anaconda/noarch::prompt-toolkit-3.0.4-py_0\n",
            "  prompt_toolkit     anaconda/noarch::prompt_toolkit-3.0.4-0\n",
            "  ptyprocess         anaconda/linux-64::ptyprocess-0.6.0-py37_0\n",
            "  pygments           anaconda/noarch::pygments-2.6.1-py_0\n",
            "  python-dateutil    anaconda/noarch::python-dateutil-2.8.1-py_0\n",
            "  pyzmq              anaconda/linux-64::pyzmq-18.1.1-py37he6710b0_0\n",
            "  tornado            anaconda/linux-64::tornado-6.0.4-py37h7b6447c_1\n",
            "  traitlets          anaconda/linux-64::traitlets-4.3.3-py37_0\n",
            "  wcwidth            anaconda/noarch::wcwidth-0.1.9-py_0\n",
            "  zeromq             anaconda/linux-64::zeromq-4.3.1-he6710b0_3\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates                                 pkgs/main --> anaconda\n",
            "  certifi                                         pkgs/main --> anaconda\n",
            "  conda                                           pkgs/main --> anaconda\n",
            "  openssl                                         pkgs/main --> anaconda\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libsodium-1.0.16     | 302 KB    | : 100% 1.0/1 [00:00<00:00,  5.18it/s]               \n",
            "pygments-2.6.1       | 687 KB    | : 100% 1.0/1 [00:00<00:00,  5.06it/s]\n",
            "jedi-0.16.0          | 769 KB    | : 100% 1.0/1 [00:00<00:00,  3.41it/s]\n",
            "tornado-6.0.4        | 649 KB    | : 100% 1.0/1 [00:00<00:00,  5.79it/s]\n",
            "certifi-2020.4.5.1   | 159 KB    | : 100% 1.0/1 [00:00<00:00, 10.68it/s]\n",
            "wcwidth-0.1.9        | 24 KB     | : 100% 1.0/1 [00:00<00:00, 25.84it/s]\n",
            "jupyter_core-4.6.3   | 75 KB     | : 100% 1.0/1 [00:00<00:00, 18.70it/s]\n",
            "openssl-1.1.1g       | 3.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.65it/s]\n",
            "ipython-7.13.0       | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.72it/s]\n",
            "traitlets-4.3.3      | 138 KB    | : 100% 1.0/1 [00:00<00:00, 13.81it/s]\n",
            "prompt-toolkit-3.0.4 | 240 KB    | : 100% 1.0/1 [00:00<00:00, 11.35it/s]\n",
            "ca-certificates-2020 | 132 KB    | : 100% 1.0/1 [00:00<00:00, 16.71it/s]\n",
            "conda-4.8.3          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.77it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 24.50it/s]\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 24.17it/s]\n",
            "backcall-0.1.0       | 20 KB     | : 100% 1.0/1 [00:00<00:00, 22.87it/s]\n",
            "entrypoints-0.3      | 12 KB     | : 100% 1.0/1 [00:00<00:00,  3.36it/s]\n",
            "ipython_genutils-0.2 | 39 KB     | : 100% 1.0/1 [00:00<00:00, 19.91it/s]\n",
            "pyzmq-18.1.1         | 522 KB    | : 100% 1.0/1 [00:00<00:00,  6.19it/s]\n",
            "ipykernel-5.1.4      | 168 KB    | : 100% 1.0/1 [00:00<00:00, 13.75it/s]\n",
            "jupyter_client-6.1.2 | 81 KB     | : 100% 1.0/1 [00:00<00:00, 17.35it/s]\n",
            "python-dateutil-2.8. | 224 KB    | : 100% 1.0/1 [00:00<00:00, 16.67it/s]\n",
            "prompt_toolkit-3.0.4 | 11 KB     | : 100% 1.0/1 [00:00<00:00, 25.38it/s]\n",
            "ptyprocess-0.6.0     | 23 KB     | : 100% 1.0/1 [00:00<00:00, 12.54it/s]\n",
            "parso-0.6.2          | 69 KB     | : 100% 1.0/1 [00:00<00:00, 20.76it/s]\n",
            "zeromq-4.3.1         | 666 KB    | : 100% 1.0/1 [00:00<00:00,  6.66it/s]\n",
            "pexpect-4.8.0        | 84 KB     | : 100% 1.0/1 [00:00<00:00,  5.24it/s]                \n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - yacs\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
            "    certifi-2020.4.5.1         |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pyyaml-5.1.2               |   py37h516909a_0         184 KB  conda-forge\n",
            "    yacs-0.1.6                 |             py_0          11 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pyyaml             conda-forge/linux-64::pyyaml-5.1.2-py37h516909a_0\n",
            "  yacs               conda-forge/noarch::yacs-0.1.6-py_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates      anaconda::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
            "  conda                        anaconda::conda-4.8.3-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi               anaconda::certifi-2020.4.5.1-py37_0 --> conda-forge::certifi-2020.4.5.1-py37hc8dfbb8_0\n",
            "  openssl               anaconda::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2020.4.5.1   | 151 KB    | : 100% 1.0/1 [00:00<00:00,  6.83it/s]               \n",
            "yacs-0.1.6           | 11 KB     | : 100% 1.0/1 [00:00<00:00, 13.47it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 27.99it/s]\n",
            "openssl-1.1.1g       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.85it/s]\n",
            "ca-certificates-2020 | 146 KB    | : 100% 1.0/1 [00:00<00:00, 17.70it/s]\n",
            "pyyaml-5.1.2         | 184 KB    | : 100% 1.0/1 [00:00<00:00, 12.85it/s]\n",
            "conda-4.8.3          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.84it/s]\n",
            "Preparing transaction: \\ \b\bdone\n",
            "Verifying transaction: / \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 13 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting scipy==1.4.1; python_version >= \"3\"\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 31 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 59.6 MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 871 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting absl-py>=0.7.0\n",
            "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 63.4 MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.8.0\n",
            "  Downloading protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 66.8 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.28.1-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (45.2.0.post20200210)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.14.1-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.8)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting rsa<4.1,>=3.1.4\n",
            "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gast, absl-py, termcolor, wrapt\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=e7c81e688dcaf7f8f6114ab6b110fc7fa835fa6e233f3e37921d4e40398f360b\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=a0fedc7fd197fbd810368b3739c2b9df8bd063e8500c5d98af33afba8e867e30\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c2297bb432af8acc832ab7ef52075039ba54bbfdf6e67fb99c8d716c8f829d65\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=70936 sha256=c3567f34c58e7e242b164d3f49f8e9d790678634370ed01ab5307a7001885480\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built gast absl-py termcolor wrapt\n",
            "Installing collected packages: tensorflow-estimator, scipy, werkzeug, markdown, protobuf, absl-py, grpcio, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, google-auth-oauthlib, tensorboard, keras-preprocessing, gast, h5py, keras-applications, astor, termcolor, google-pasta, opt-einsum, wrapt, tensorflow\n",
            "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.1.0 gast-0.2.2 google-auth-1.14.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "grpc",
                  "pyasn1",
                  "pyasn1_modules",
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/site-packages (from tensorboardX) (3.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from tensorboardX) (1.18.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from tensorboardX) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0.post20200210)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=35100b082bc42d2738be1405acbe80fdcfdf0515693a267c82e9d3761a5e25cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: future, ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0 future-0.18.2\n",
            "Collecting face_alignment\n",
            "  Downloading face_alignment-1.0.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from face_alignment) (4.42.1)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/site-packages (from face_alignment) (1.4.1)\n",
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 43 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (from face_alignment) (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from face_alignment) (1.18.1)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting matplotlib!=3.0.0,>=2.0.0\n",
            "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 63.9 MB/s \n",
            "\u001b[?25hCollecting PyWavelets>=0.4.0\n",
            "  Downloading PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 58.7 MB/s \n",
            "\u001b[?25hCollecting networkx>=2.0\n",
            "  Downloading networkx-2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/site-packages (from scikit-image->face_alignment) (7.0.0)\n",
            "Collecting imageio>=2.3.0\n",
            "  Downloading imageio-2.8.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from torch->face_alignment) (0.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (2.8.1)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->face_alignment) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.14.0)\n",
            "Installing collected packages: kiwisolver, cycler, pyparsing, matplotlib, PyWavelets, networkx, imageio, scikit-image, opencv-python, face-alignment\n",
            "Successfully installed PyWavelets-1.1.1 cycler-0.10.0 face-alignment-1.0.0 imageio-2.8.0 kiwisolver-1.2.0 matplotlib-3.2.1 networkx-2.4 opencv-python-4.2.0.34 pyparsing-2.4.7 scikit-image-0.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fToL6OKn2qaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1f5396bf-3eab-4c9b-dc85-0de8025a0425"
      },
      "source": [
        "# # 1，云盘源码安装（首次需要，之后使用无需再装）\n",
        "\n",
        "# !mkdir -p /content/drive/My\\ Drive/fsganLab/fshome\n",
        "# %cd /content/drive/My Drive/fsganLab/fshome\n",
        "\n",
        "# !git clone https://github.com/YuvalNirkin/fsgan.git\n",
        "# !git clone https://github.com/YuvalNirkin/face_detection_dsfd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fsganLab/fshome\n",
            "Cloning into 'fsgan'...\n",
            "remote: Enumerating objects: 325, done.\u001b[K\n",
            "remote: Counting objects: 100% (325/325), done.\u001b[K\n",
            "remote: Compressing objects: 100% (218/218), done.\u001b[K\n",
            "remote: Total 325 (delta 167), reused 254 (delta 101), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (325/325), 9.59 MiB | 16.55 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "Cloning into 'face_detection_dsfd'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 308 (delta 4), reused 7 (delta 2), pack-reused 298\u001b[K\n",
            "Receiving objects: 100% (308/308), 17.65 MiB | 24.39 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2RLXSOPBEnZ",
        "colab_type": "code",
        "outputId": "d9004b59-3556-4756-f348-d8cd8eb778c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/fsganLab/fshome"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fsganLab/fshome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNbgTpabri-g",
        "colab_type": "code",
        "outputId": "a129216f-289a-4bcf-b923-6c7064496b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# # 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共15个模型文件\n",
        "# # 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共15个模型文件\n",
        "# # 问官方要的下载文件授权，放在fshome下，运行得到权重文件夹 weights，里面共15个模型文件\n",
        "\n",
        "# # 首次安装需要，之后可以注释掉\n",
        "# !python download_fsgan_models.py -m v2\n",
        "# # 由于 colab 空格问题，重新组织下目录结构\n",
        "# !mv /content/drive/My\\ Drive/fsganLab/fshome/weights /content/drive/My\\ Drive/fsganLab\n",
        "# !cp /content/drive/My\\ Drive/fsganLab/fshome/fsgan/inference/swap.py ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/9] Downloading \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
            "nfv_msrunet_256_1_2_reenactment_v2.1.pth: 304MB [00:03, 92.2MB/s]\n",
            "[2/9] Downloading \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
            "ijbc_msrunet_256_1_2_inpainting_v2.pth: 297MB [00:03, 76.4MB/s]\n",
            "[3/9] Downloading \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n",
            "ijbc_msrunet_256_1_2_blending_v2.pth: 211MB [00:03, 52.8MB/s]\n",
            "[4/9] Downloading \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
            "celeba_unet_256_1_2_segmentation_v2.pth: 116MB [00:01, 58.3MB/s]\n",
            "[5/9] Downloading \"WIDERFace_DSFD_RES152.pth\"...\n",
            "WIDERFace_DSFD_RES152.pth: 481MB [00:04, 119MB/s]\n",
            "[6/9] Downloading \"hr18_wflw_landmarks.pth\"...\n",
            "hr18_wflw_landmarks.pth: 100% 39.2M/39.2M [00:00<00:00, 79.0MB/s]\n",
            "[7/9] Downloading \"vggface2_vgg19_256_1_2_id.pth\"...\n",
            "vggface2_vgg19_256_1_2_id.pth: 100% 823M/823M [00:08<00:00, 97.8MB/s]\n",
            "[8/9] Downloading \"celeba_vgg19_256_2_0_28_attr.pth\"...\n",
            "celeba_vgg19_256_2_0_28_attr.pth: 100% 685M/685M [00:13<00:00, 49.3MB/s]\n",
            "[9/9] Downloading \"hopenet_robust_alpha1.pth\"...\n",
            "hopenet_robust_alpha1.pth: 100% 95.9M/95.9M [00:00<00:00, 105MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tUPOI-MDY1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 注册 fshome\n",
        "!echo 'export PYTHONPATH=$PYTHONPATH:/content/drive/My\\ Drive/fsganLab/fshome' >> ~/.profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gfJfjQxMPVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!source ~/.profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a96GLQqM2NZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 编译安装 opencv (如果要 avc1 编码则需要，无比费时，容易出错)\n",
        "# !wget -O opencv.zip https://github.com/opencv/opencv/archive/4.3.0.zip\n",
        "# !unzip opencv.zip\n",
        "# !rm opencv.zip\n",
        "# %cd /content/opencv-4.3.0\n",
        "# !mkdir build\n",
        "# %cd /content/opencv-4.3.0/build\n",
        "# !cmake -D CMAKE_BUILD_TYPE=RELEASE \\\n",
        "# -D CMAKE_INSTALL_PREFIX=$(python -c \"import sys; print(sys.prefix)\") \\\n",
        "# -D ENABLE_FAST_MATH=ON \\\n",
        "# -D INSTALL_PYTHON_EXAMPLES=OFF \\\n",
        "# -D INSTALL_C_EXAMPLES=OFF \\\n",
        "# -D OPENCV_ENABLE_NONFREE=OFF \\\n",
        "# -D CMAKE_INSTALL_PREFIX=$(python -c \"import sys; print(sys.prefix)\") \\\n",
        "# -D PYTHON_EXECUTABLE=$(which python) \\\n",
        "# -D PYTHON_INCLUDE_DIR=$(python -c \"from distutils.sysconfig import get_python_inc; print(get_python_inc())\") \\\n",
        "# -D PYTHON_PACKAGES_PATH=$(python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\") \\\n",
        "# -D BUILD_EXAMPLES=OFF \\\n",
        "# -D BUILD_JPEG=ON ..\n",
        "# !make -j8\n",
        "# !sudo make install\n",
        "# !sudo ldconfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR5vQfn9s0op",
        "colab_type": "text"
      },
      "source": [
        "# 修改五个文件，防止因为没有编码器报错（你只需要按就 ok）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN1QRFC_szFY",
        "colab_type": "code",
        "outputId": "8caba4ea-b416-4221-fc6c-ca5b80913f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/fsganLab/fshome"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fsganLab/fshome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WwK-FEns97c",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "2e87a8f6-c08d-4113-c216-d7e5917ae197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title 1，使用 mp4v 需要修改 fsgan/utils/video_renderer.py\n",
        "%%writefile fsgan/utils/video_renderer.py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from fsgan.utils.img_utils import tensor2bgr\n",
        "from fsgan.utils.bbox_utils import crop2img, scale_bbox\n",
        "\n",
        "\n",
        "class VideoRenderer(mp.Process):\n",
        "    \"\"\" Renders input video frames to both screen and video file.\n",
        "\n",
        "    For more control on the rendering, this class should be inherited from and the on_render method overridden\n",
        "    with an application specific implementation.\n",
        "\n",
        "    Args:\n",
        "        display (bool): If True, the rendered video will be displayed on screen\n",
        "        verbose (int): Verbose level. Controls the amount of debug information in the rendering\n",
        "        verbose_size (tuple of int): The rendered frame size for verbose level other than zero (width, height)\n",
        "        output_crop (bool): If True, a cropped frame of size (resolution, resolution) will be rendered for\n",
        "            verbose level zero\n",
        "        resolution (int): Determines the size of cropped frames to be (resolution, resolution)\n",
        "        crop_scale (float): Multiplier factor to scale tight bounding boxes\n",
        "        encoder_codec (str) Encoder codec code\n",
        "    \"\"\"\n",
        "    def __init__(self, display=False, verbose=0, verbose_size=None, output_crop=False, resolution=256, crop_scale=1.2,\n",
        "                 encoder_codec='mp4v'):\n",
        "        super(VideoRenderer, self).__init__()\n",
        "        self._display = display\n",
        "        self._verbose = verbose\n",
        "        self._verbose_size = verbose_size\n",
        "        self._output_crop = output_crop\n",
        "        self._resolution = resolution\n",
        "        self._crop_scale = crop_scale\n",
        "        self._running = True\n",
        "        self._input_queue = mp.Queue()\n",
        "        self._reply_queue = mp.Queue()\n",
        "        self._fourcc = cv2.VideoWriter_fourcc(*encoder_codec)\n",
        "        self._in_vid = None\n",
        "        self._out_vid = None\n",
        "        self._seq = None\n",
        "        self._in_vid_path = None\n",
        "        self._total_frames = None\n",
        "        self._frame_count = 0\n",
        "\n",
        "    def init(self, in_vid_path, seq, out_vid_path=None, **kwargs):\n",
        "        \"\"\" Initialize the video render for a new video rendering job.\n",
        "\n",
        "        Args:\n",
        "            in_vid_path (str): Input video path\n",
        "            seq (Sequence): Input sequence corresponding to the input video\n",
        "            out_vid_path (str, optional): If specified, the rendering will be written to an output video in that path\n",
        "            **kwargs (dict): Additional keyword arguments that will be added as members of the class. This allows\n",
        "                inheriting classes to access those arguments from the new process\n",
        "        \"\"\"\n",
        "        self._input_queue.put([in_vid_path, seq, out_vid_path, kwargs])\n",
        "\n",
        "    def write(self, *args):\n",
        "        \"\"\" Add tensors for rendering.\n",
        "\n",
        "        Args:\n",
        "            *args (tuple of torch.Tensor): The tensors for rendering\n",
        "        \"\"\"\n",
        "        self._input_queue.put([a.cpu() for a in args])\n",
        "\n",
        "    def wait_until_finished(self):\n",
        "        \"\"\" Wait for the video renderer to finish the current video rendering job. \"\"\"\n",
        "        return self._reply_queue.get()\n",
        "\n",
        "    def on_render(self, *args):\n",
        "        \"\"\" Given the input tensors this method produces a cropped rendered image.\n",
        "\n",
        "        This method should be overridden by inheriting classes to customize the rendering. By default this method\n",
        "        expects the first tensor to be a cropped image tensor of shape (B, 3, H, W) where B is the batch size,\n",
        "        H is the height of the image and W is the width of the image.\n",
        "\n",
        "        Args:\n",
        "            *args (tuple of torch.Tensor): The tensors for rendering\n",
        "\n",
        "        Returns:\n",
        "            render_bgr (np.array): The cropped rendered image\n",
        "        \"\"\"\n",
        "        return tensor2bgr(args[0])\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\" Main processing loop. Intended to be executed on a separate process. \"\"\"\n",
        "        while self._running:\n",
        "            task = self._input_queue.get()\n",
        "\n",
        "            # Initialize new video rendering task\n",
        "            if self._in_vid is None:\n",
        "                self._in_vid_path, self._seq, out_vid_path = task[:3]\n",
        "                additional_attributes = task[3]\n",
        "                self._frame_count = 0\n",
        "\n",
        "                # Add additional arguments as members\n",
        "                for attr_name, attr_val in additional_attributes.items():\n",
        "                    setattr(self, attr_name, attr_val)\n",
        "\n",
        "                # Open input video\n",
        "                self._in_vid = cv2.VideoCapture(self._in_vid_path)\n",
        "                assert self._in_vid.isOpened(), f'Failed to open video: \"{self._in_vid_path}\"'\n",
        "\n",
        "                in_total_frames = int(self._in_vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                fps = self._in_vid.get(cv2.CAP_PROP_FPS)\n",
        "                in_vid_width = int(self._in_vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                in_vid_height = int(self._in_vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                self._total_frames = in_total_frames if self._verbose == 0 else len(self._seq)\n",
        "                # print(f'Debug: initializing video: \"{self._in_vid_path}\", total_frames={self._total_frames}')\n",
        "\n",
        "                # Initialize output video\n",
        "                if out_vid_path is not None:\n",
        "                    out_size = (in_vid_width, in_vid_height)\n",
        "                    if self._verbose <= 0 and self._output_crop:\n",
        "                        out_size = (self._resolution, self._resolution)\n",
        "                    elif self._verbose_size is not None:\n",
        "                        out_size = self._verbose_size\n",
        "                    self._out_vid = cv2.VideoWriter(out_vid_path, self._fourcc, fps, out_size)\n",
        "\n",
        "                # Write frames as they are until the start of the sequence\n",
        "                if self._verbose == 0:\n",
        "                    for i in range(self._seq.start_index):\n",
        "                        # Read frame\n",
        "                        ret, frame_bgr = self._in_vid.read()\n",
        "                        assert frame_bgr is not None, f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "                        self._render(frame_bgr)\n",
        "                        self._frame_count += 1\n",
        "\n",
        "                continue\n",
        "\n",
        "            # Write a batch of frames\n",
        "            tensors = task\n",
        "            batch_size = tensors[0].shape[0]\n",
        "\n",
        "            # For each frame in the current batch of tensors\n",
        "            for b in range(batch_size):\n",
        "                # Handle full frames if output_crop was not specified\n",
        "                full_frame_bgr, bbox = None, None\n",
        "                if self._verbose == 0 and not self._output_crop:\n",
        "                    # Read frame from input video\n",
        "                    ret, full_frame_bgr = self._in_vid.read()\n",
        "                    assert full_frame_bgr is not None, \\\n",
        "                        f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "\n",
        "                    # Get bounding box from sequence\n",
        "                    det = self._seq[self._frame_count - self._seq.start_index]\n",
        "                    bbox = np.concatenate((det[:2], det[2:] - det[:2]))\n",
        "                    bbox = scale_bbox(bbox, self._crop_scale)\n",
        "\n",
        "                render_bgr = self.on_render(*[t[b] for t in tensors])\n",
        "                self._render(render_bgr, full_frame_bgr, bbox)\n",
        "                self._frame_count += 1\n",
        "                # print(f'Debug: Writing frame: {self._frame_count}')\n",
        "\n",
        "            # Check if we reached the end of the sequence\n",
        "            if self._verbose == 0 and self._frame_count >= (self._seq.start_index + len(self._seq)):\n",
        "                for i in range(self._seq.start_index + len(self._seq), self._total_frames):\n",
        "                    # Read frame\n",
        "                    ret, frame_bgr = self._in_vid.read()\n",
        "                    assert frame_bgr is not None, f'Failed to read frame {i} from input video: \"{self._in_vid_path}\"'\n",
        "                    self._render(frame_bgr)\n",
        "                    self._frame_count += 1\n",
        "\n",
        "            # Check if all frames have been processed\n",
        "            if self._frame_count >= self._total_frames:\n",
        "                # Clean up\n",
        "                self._in_vid.release()\n",
        "                self._out_vid.release()\n",
        "                self._in_vid = None\n",
        "                self._out_vid = None\n",
        "                self._seq = None\n",
        "                self._in_vid_path = None\n",
        "                self._total_frames = None\n",
        "                self._frame_count = 0\n",
        "\n",
        "                # Notify job is finished\n",
        "                self._reply_queue.put(True)\n",
        "\n",
        "    def _render(self, render_bgr, full_frame_bgr=None, bbox=None):\n",
        "        if self._verbose == 0 and not self._output_crop and full_frame_bgr is not None:\n",
        "            render_bgr = crop2img(full_frame_bgr, render_bgr, bbox)\n",
        "        if self._out_vid is not None:\n",
        "            self._out_vid.write(render_bgr)\n",
        "        if self._display:\n",
        "            cv2.imshow('render', render_bgr)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                self._running = False"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting fsgan/utils/video_renderer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTD3d-1StZjn",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "73b0f6c6-c262-41bb-a05a-90c0208dd304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title 2，使用 mp4v 需要修改 fsgan/preprocess/preprocess_video.py\n",
        "%%writefile fsgan/preprocess/preprocess_video.py\n",
        "\"\"\" Video preprocessing.\n",
        "\n",
        "This script implements all preprocessing required for both training and inference.\n",
        "The preprocessing information will be cached in a directory by the file's name without the extension,\n",
        "residing in the same directory as the file. The information contains: face detections, face sequences,\n",
        "and cropped videos per sequence. In addition for each cropped video, the corresponding pose, landmarks, and\n",
        "segmentation masks will be computed and cached.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from face_detection_dsfd.face_detector import FaceDetector\n",
        "from fsgan.utils.utils import set_device, load_model\n",
        "from fsgan.preprocess.detections2sequences_center import main as detections2sequences_main\n",
        "from fsgan.preprocess.crop_video_sequences import main as crop_video_sequences_main\n",
        "from fsgan.preprocess.crop_image_sequences import main as crop_image_sequences_main\n",
        "from fsgan.datasets.video_inference_dataset import VideoInferenceDataset\n",
        "import fsgan.datasets.img_landmarks_transforms as img_landmarks_transforms\n",
        "from fsgan.datasets.img_landmarks_transforms import Resize, ToTensor\n",
        "from fsgan.utils.temporal_smoothing import TemporalSmoothing\n",
        "from fsgan.utils.landmarks_utils import LandmarksHeatMapEncoder, smooth_landmarks_98pts\n",
        "from fsgan.utils.seg_utils import encode_binary_mask, remove_inner_mouth\n",
        "from fsgan.utils.batch import main as batch\n",
        "\n",
        "\n",
        "base_parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n",
        "\n",
        "general = base_parser.add_argument_group('general')\n",
        "general.add_argument('-r', '--resolution', default=256, type=int, metavar='N',\n",
        "                     help='finest processing resolution')\n",
        "general.add_argument('-cs', '--crop_scale', default=1.2, type=float, metavar='F',\n",
        "                     help='crop scale relative to bounding box')\n",
        "general.add_argument('--gpus', default=None, nargs='+', type=int, metavar='N',\n",
        "                     help='list of gpu ids to use')\n",
        "general.add_argument('--cpu_only', action='store_true',\n",
        "                     help='force cpu only')\n",
        "general.add_argument('-d', '--display', action='store_true',\n",
        "                     help='display the rendering')\n",
        "general.add_argument('-v', '--verbose', default=0, type=int, metavar='N',\n",
        "                     help='verbose level')\n",
        "general.add_argument('-ec', '--encoder_codec', default='mp4v', metavar='STR',\n",
        "                     help='encoder codec code')\n",
        "\n",
        "detection = base_parser.add_argument_group('detection')\n",
        "detection.add_argument('-dm', '--detection_model', metavar='PATH', default='../weights/WIDERFace_DSFD_RES152.pth',\n",
        "                       help='path to face detection model')\n",
        "detection.add_argument('-db', '--det_batch_size', default=8, type=int, metavar='N',\n",
        "                       help='detection batch size')\n",
        "detection.add_argument('-dp', '--det_postfix', default='_dsfd.pkl', metavar='POSTFIX',\n",
        "                       help='detection file postfix')\n",
        "\n",
        "sequences = base_parser.add_argument_group('sequences')\n",
        "sequences.add_argument('-it', '--iou_thresh', default=0.75, type=float,\n",
        "                       metavar='F', help='IOU threshold')\n",
        "sequences.add_argument('-ml', '--min_length', default=10, type=int,\n",
        "                       metavar='N', help='minimum sequence length')\n",
        "sequences.add_argument('-ms', '--min_size', default=64, type=int,\n",
        "                       metavar='N', help='minimum sequence average bounding box size')\n",
        "sequences.add_argument('-ck', '--center_kernel', default=25, type=int,\n",
        "                       metavar='N', help='center average kernel size')\n",
        "sequences.add_argument('-sk', '--size_kernel', default=51, type=int,\n",
        "                       metavar='N', help='size average kernel size')\n",
        "sequences.add_argument('-dsd', '--disable_smooth_det', dest='smooth_det', action='store_false',\n",
        "                       help='disable smoothing the detection bounding boxes')\n",
        "sequences.add_argument('-sp', '--seq_postfix', default='_dsfd_seq.pkl', metavar='POSTFIX',\n",
        "                       help='sequence file postfix')\n",
        "sequences.add_argument('-we', '--write_empty', action='store_true',\n",
        "                       help='write empty sequence lists to file')\n",
        "\n",
        "pose = base_parser.add_argument_group('pose')\n",
        "pose.add_argument('-pm', '--pose_model', default='../weights/hopenet_robust_alpha1.pth', metavar='PATH',\n",
        "                       help='path to face pose model file')\n",
        "pose.add_argument('-pb', '--pose_batch_size', default=128, type=int, metavar='N',\n",
        "                       help='pose batch size')\n",
        "pose.add_argument('-pp', '--pose_postfix', default='_pose.npz', metavar='POSTFIX',\n",
        "                       help='pose file postfix')\n",
        "pose.add_argument('-cp', '--cache_pose', action='store_true',\n",
        "                  help='Toggle whether to cache pose')\n",
        "pose.add_argument('-cf', '--cache_frontal', action='store_true',\n",
        "                  help='Toggle whether to cache frontal images for each sequence')\n",
        "pose.add_argument('-spo', '--smooth_poses', default=5, type=int, metavar='N',\n",
        "                  help='poses temporal smoothing kernel size')\n",
        "\n",
        "landmarks = base_parser.add_argument_group('landmarks')\n",
        "landmarks.add_argument('-lm', '--lms_model', default='../weights/hr18_wflw_landmarks.pth', metavar='PATH',\n",
        "                       help='landmarks model')\n",
        "landmarks.add_argument('-lb', '--lms_batch_size', default=64, type=int, metavar='N',\n",
        "                       help='landmarks batch size')\n",
        "landmarks.add_argument('-lp', '--landmarks_postfix', default='_lms.npz', metavar='POSTFIX',\n",
        "                       help='landmarks file postfix')\n",
        "landmarks.add_argument('-cl', '--cache_landmarks', action='store_true',\n",
        "                       help='Toggle whether to cache landmarks')\n",
        "landmarks.add_argument('-sl', '--smooth_landmarks', default=7, type=int, metavar='N',\n",
        "                       help='landmarks temporal smoothing kernel size')\n",
        "\n",
        "segmentation = base_parser.add_argument_group('segmentation')\n",
        "segmentation.add_argument('-sm', '--seg_model', default='../weights/celeba_unet_256_1_2_segmentation_v2.pth',\n",
        "                          metavar='PATH', help='segmentation model')\n",
        "segmentation.add_argument('-sb', '--seg_batch_size', default=32, type=int, metavar='N',\n",
        "                          help='segmentation batch size')\n",
        "segmentation.add_argument('-sep', '--segmentation_postfix', default='_seg.pkl', metavar='POSTFIX',\n",
        "                          help='segmentation file postfix')\n",
        "segmentation.add_argument('-cse', '--cache_segmentation', action='store_true',\n",
        "                          help='Toggle whether to cache segmentation')\n",
        "segmentation.add_argument('-sse', '--smooth_segmentation', default=5, type=int, metavar='N',\n",
        "                          help='segmentation temporal smoothing kernel size')\n",
        "segmentation.add_argument('-srm', '--seg_remove_mouth', action='store_true',\n",
        "                          help='if true, the inner part of the mouth will be removed from the segmentation')\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "                                 parents=[base_parser])\n",
        "parser.add_argument('input', metavar='VIDEO', nargs='+',\n",
        "                    help='path to input video')\n",
        "parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                    help='output directory')\n",
        "d = parser.get_default\n",
        "\n",
        "\n",
        "class VideoProcessBase(object):\n",
        "    def __init__(self, resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth')):\n",
        "        # General\n",
        "        self.resolution = resolution\n",
        "        self.crop_scale = crop_scale\n",
        "        self.display = display\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Detection\n",
        "        self.face_detector = FaceDetector(det_postfix, detection_model, gpus, det_batch_size, display)\n",
        "        self.det_postfix = det_postfix\n",
        "\n",
        "        # Sequences\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.min_length = min_length\n",
        "        self.min_size = min_size\n",
        "        self.center_kernel = center_kernel\n",
        "        self.size_kernel = size_kernel\n",
        "        self.smooth_det = smooth_det\n",
        "        self.seq_postfix = seq_postfix\n",
        "        self.write_empty = write_empty\n",
        "\n",
        "        # Pose\n",
        "        self.pose_batch_size = pose_batch_size\n",
        "        self.pose_postfix = pose_postfix\n",
        "        self.cache_pose = cache_pose\n",
        "        self.cache_frontal = cache_frontal\n",
        "        self.smooth_poses = smooth_poses\n",
        "\n",
        "        # Landmarks\n",
        "        self.smooth_landmarks = smooth_landmarks\n",
        "        self.landmarks_postfix = landmarks_postfix\n",
        "        self.cache_landmarks = cache_landmarks\n",
        "        self.lms_batch_size = lms_batch_size\n",
        "\n",
        "        # Segmentation\n",
        "        self.smooth_segmentation = smooth_segmentation\n",
        "        self.segmentation_postfix = segmentation_postfix\n",
        "        self.cache_segmentation = cache_segmentation\n",
        "        self.seg_batch_size = seg_batch_size\n",
        "        self.seg_remove_mouth = seg_remove_mouth and cache_landmarks\n",
        "\n",
        "        # Initialize device\n",
        "        torch.set_grad_enabled(False)\n",
        "        self.device, self.gpus = set_device(gpus, not cpu_only)\n",
        "\n",
        "        # Load models\n",
        "        self.face_pose = load_model(pose_model, 'face pose', self.device) if cache_pose else None\n",
        "        self.L = load_model(lms_model, 'face landmarks', self.device) if cache_landmarks else None\n",
        "        self.S = load_model(seg_model, 'face segmentation', self.device) if cache_segmentation else None\n",
        "\n",
        "        # Initialize heatmap encoder\n",
        "        self.heatmap_encoder = LandmarksHeatMapEncoder().to(self.device)\n",
        "\n",
        "        # Initialize normalization tensors\n",
        "        # Note: this is necessary because of the landmarks model\n",
        "        self.img_mean = torch.as_tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)\n",
        "        self.img_std = torch.as_tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)\n",
        "        self.context_mean = torch.as_tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n",
        "        self.context_std = torch.as_tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n",
        "\n",
        "        # Support multiple GPUs\n",
        "        if self.gpus and len(self.gpus) > 1:\n",
        "            self.face_pose = nn.DataParallel(self.face_pose, self.gpus) if self.face_pose is not None else None\n",
        "            self.L = nn.DataParallel(self.L, self.gpus) if self.L is not None else None\n",
        "            self.S = nn.DataParallel(self.S, self.gpus) if self.S is not None else None\n",
        "\n",
        "        # Initialize temportal smoothing\n",
        "        if smooth_segmentation > 0:\n",
        "            self.smooth_seg = TemporalSmoothing(3, smooth_segmentation).to(self.device)\n",
        "        else:\n",
        "            self.smooth_seg = None\n",
        "\n",
        "        # Initialize output videos format\n",
        "        self.encoder_codec = encoder_codec\n",
        "        self.fourcc = cv2.VideoWriter_fourcc(*encoder_codec)\n",
        "\n",
        "    def process_pose(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_pose:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            Resize(224), ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_pose_path = os.path.splitext(curr_vid_path)[0] + self.pose_postfix\n",
        "\n",
        "            if os.path.isfile(curr_pose_path):\n",
        "                continue\n",
        "            print('=> Computing face poses for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.pose_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            seq_poses = []\n",
        "            for i, frame in enumerate(tqdm(in_vid_loader, unit='batches', file=sys.stdout)):\n",
        "                frame = frame.to(self.device)\n",
        "                poses = self.face_pose(frame).div_(99.)  # Yaw, Pitch, Roll\n",
        "                seq_poses.append(poses.cpu().numpy())\n",
        "            seq_poses = np.concatenate(seq_poses)\n",
        "\n",
        "            # Save landmarks to file\n",
        "            seq_landmarks_smoothed = smooth_poses(seq_poses, self.smooth_poses)\n",
        "            np.savez_compressed(curr_pose_path, poses=seq_poses, poses_smoothed=seq_landmarks_smoothed)\n",
        "\n",
        "    def extract_frontal_images(self, input_path, output_dir, seq_file_path, out_postfix='.jpg', resolution=None):\n",
        "        if not self.cache_frontal:\n",
        "            return\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d.mp4' % seq.id\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_pose_path = os.path.splitext(curr_vid_path)[0] + self.pose_postfix\n",
        "            curr_frontal_path = os.path.splitext(curr_vid_path)[0] + out_postfix\n",
        "\n",
        "            if os.path.isfile(curr_frontal_path):\n",
        "                continue\n",
        "\n",
        "            # Open current video file\n",
        "            vid = cv2.VideoCapture(curr_vid_path)\n",
        "            if not vid.isOpened():\n",
        "                raise RuntimeError('Failed to read video: ' + curr_vid_path)\n",
        "\n",
        "            # Load current sequence poses\n",
        "            curr_poses = np.load(curr_pose_path)['poses_smoothed']\n",
        "\n",
        "            # Read frontal image from video\n",
        "            frontal_index = np.argmin(np.linalg.norm(curr_poses, axis=1))\n",
        "            vid.set(cv2.CAP_PROP_POS_FRAMES, frontal_index)\n",
        "            ret, frontal_bgr = vid.read()\n",
        "\n",
        "            # Resize image\n",
        "            if resolution is not None:\n",
        "                frontal_bgr = cv2.resize(frontal_bgr, (resolution, resolution), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Write frontal image to file\n",
        "            cv2.imwrite(curr_frontal_path, frontal_bgr)\n",
        "\n",
        "    def process_landmarks(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_landmarks:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_lms_path = os.path.splitext(curr_vid_path)[0] + self.landmarks_postfix\n",
        "\n",
        "            if os.path.isfile(curr_lms_path):\n",
        "                continue\n",
        "            print('=> Computing face landmarks for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.lms_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            seq_landmarks = []\n",
        "            for i, frame in enumerate(tqdm(in_vid_loader, unit='batches', file=sys.stdout)):\n",
        "                frame = frame.to(self.device)\n",
        "                H = self.L(frame)\n",
        "                landmarks = self.heatmap_encoder(H)\n",
        "                seq_landmarks.append(landmarks.cpu().numpy())\n",
        "            seq_landmarks = np.concatenate(seq_landmarks)\n",
        "\n",
        "            # Save landmarks to file\n",
        "            seq_landmarks_smoothed = smooth_landmarks_98pts(seq_landmarks, self.smooth_landmarks)\n",
        "            np.savez_compressed(curr_lms_path, landmarks=seq_landmarks, landmarks_smoothed=seq_landmarks_smoothed)\n",
        "\n",
        "    def process_segmentation(self, input_path, output_dir, seq_file_path):\n",
        "        if not self.cache_segmentation:\n",
        "            return\n",
        "        input_path_no_ext, input_ext = os.path.splitext(input_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            seq_list = pickle.load(fp)\n",
        "\n",
        "        # Initialize transforms\n",
        "        img_transforms = img_landmarks_transforms.Compose([\n",
        "            ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "        # For each sequence\n",
        "        for seq in seq_list:\n",
        "            curr_vid_name = os.path.basename(input_path_no_ext) + '_seq%02d%s' % (seq.id, input_ext)\n",
        "            curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "            curr_seg_path = os.path.splitext(curr_vid_path)[0] + self.segmentation_postfix\n",
        "\n",
        "            if self.seg_remove_mouth:\n",
        "                curr_lms_path = os.path.splitext(curr_vid_path)[0] + self.landmarks_postfix\n",
        "                landmarks = np.load(curr_lms_path)['landmarks_smoothed']\n",
        "                frame_count = 0\n",
        "\n",
        "            if os.path.isfile(curr_seg_path):\n",
        "                continue\n",
        "            print('=> Computing face segmentation for video: \"%s\"...' % curr_vid_name)\n",
        "\n",
        "            # Initialize input video\n",
        "            in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
        "            in_vid_loader = DataLoader(in_vid, batch_size=self.seg_batch_size, num_workers=1, pin_memory=True,\n",
        "                                       drop_last=False, shuffle=False)\n",
        "\n",
        "            # For each batch of frames in the input video\n",
        "            pbar = tqdm(in_vid_loader, unit='batches', file=sys.stdout)\n",
        "            prev_segmentation = None\n",
        "            r = self.smooth_seg.kernel_radius\n",
        "            encoded_segmentations = []\n",
        "            pad_prev, pad_next = r, r   # This initialization is only relevant if there is a leftover from last batch\n",
        "            for i, frame in enumerate(pbar):\n",
        "                frame = frame.to(self.device)\n",
        "\n",
        "                # Compute segmentation\n",
        "                raw_segmentation = self.S(frame)\n",
        "                segmentation = torch.cat((prev_segmentation, raw_segmentation), dim=0) \\\n",
        "                    if prev_segmentation is not None else raw_segmentation\n",
        "                if segmentation.shape[0] > r:\n",
        "                    pad_prev, pad_next = r if prev_segmentation is None else 0, min(r, self.seg_batch_size - frame.shape[0])\n",
        "                    segmentation = self.smooth_seg(segmentation, pad_prev=pad_prev, pad_next=pad_next)\n",
        "\n",
        "                    # Note: the pad_next value here is only relevant if there is a leftover from last batch\n",
        "                    prev_segmentation = raw_segmentation[-(r * 2 - pad_next):]\n",
        "\n",
        "                mask = segmentation.argmax(1) == 1\n",
        "\n",
        "                # Encode segmentation\n",
        "                for b in range(mask.shape[0]):\n",
        "                    curr_mask = mask[b].cpu().numpy()\n",
        "                    if self.seg_remove_mouth:\n",
        "                        curr_mask = remove_inner_mouth(curr_mask, landmarks[frame_count])\n",
        "                        frame_count += 1\n",
        "                    encoded_segmentations.append(encode_binary_mask(curr_mask))\n",
        "\n",
        "            # Final iteration if we have leftover unsmoothed segmentations from the last batch\n",
        "            if pad_next < r:\n",
        "                # Compute segmentation\n",
        "                segmentation = self.smooth_seg(prev_segmentation, pad_prev=pad_prev, pad_next=r)\n",
        "                mask = segmentation.argmax(1) == 1\n",
        "\n",
        "                # Encode segmentation\n",
        "                for b in range(mask.shape[0]):\n",
        "                    curr_mask = mask[b].cpu().numpy()\n",
        "                    if self.seg_remove_mouth:\n",
        "                        curr_mask = remove_inner_mouth(curr_mask, landmarks[frame_count])\n",
        "                        frame_count += 1\n",
        "                    encoded_segmentations.append(encode_binary_mask(curr_mask))\n",
        "\n",
        "            # Write to file\n",
        "            with open(curr_seg_path, \"wb\") as fp:  # Pickling\n",
        "                pickle.dump(encoded_segmentations, fp)\n",
        "\n",
        "\n",
        "    def cache(self, input_path, output_dir=None):\n",
        "        # Validation\n",
        "        assert os.path.isfile(input_path), 'Input path \"%s\" does not exist' % input_path\n",
        "        assert output_dir is None or os.path.isdir(output_dir), 'Output path \"%s\" must be a directory' % output_dir\n",
        "        is_vid = os.path.splitext(input_path)[1] == '.mp4'\n",
        "\n",
        "        # Set paths\n",
        "        output_dir = os.path.splitext(input_path)[0] if output_dir is None else output_dir\n",
        "        det_file_path = os.path.splitext(input_path)[0] + self.det_postfix\n",
        "        if not os.path.isfile(det_file_path):   # Check if there is a detection file in the same directory as the video\n",
        "            det_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] +\n",
        "                                         self.det_postfix)\n",
        "        seq_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + self.seq_postfix)\n",
        "        first_cropped_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] +\n",
        "                                          '_seq00' + os.path.splitext(input_path)[1])\n",
        "        pose_file_path = os.path.join(output_dir, os.path.splitext(os.path.basename(input_path))[0] + self.pose_postfix)\n",
        "\n",
        "        # Create directory\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "\n",
        "        # Face detection\n",
        "        if not os.path.isfile(det_file_path):\n",
        "            self.face_detector(input_path, det_file_path)\n",
        "\n",
        "        # Detections to sequences\n",
        "        if not os.path.isfile(seq_file_path):\n",
        "            detections2sequences_main(input_path, seq_file_path, det_file_path, self.iou_thresh, self.min_length,\n",
        "                                      self.min_size, self.crop_scale, self.center_kernel, self.size_kernel,\n",
        "                                      self.smooth_det, self.display, self.write_empty)\n",
        "\n",
        "        # Crop video sequences\n",
        "        if not os.path.isfile(first_cropped_path):\n",
        "            if is_vid:\n",
        "                crop_video_sequences_main(input_path, output_dir, seq_file_path, self.seq_postfix, self.resolution,\n",
        "                                          self.crop_scale, select='all', disable_tqdm=False,\n",
        "                                          encoder_codec=self.encoder_codec)\n",
        "            else:\n",
        "                crop_image_sequences_main(input_path, output_dir, seq_file_path, self.seq_postfix, '.jpg',\n",
        "                                          self.resolution, self.crop_scale)\n",
        "\n",
        "        # Face poses\n",
        "        # if not os.path.isfile(pose_file_path) and is_vid:\n",
        "        #     self.process_pose(input_path, output_dir, seq_file_path, pose_file_path)\n",
        "        # if is_vid:\n",
        "        self.process_pose(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Extract frontal images\n",
        "        # if self.cache_pose and self.cache_frontal and is_vid:\n",
        "        #     self.extract_frontal_images(input_path, output_dir, pose_file_path)\n",
        "        if self.cache_pose and self.cache_frontal and is_vid:\n",
        "            self.extract_frontal_images(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Cache landmarks\n",
        "        self.process_landmarks(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        # Cache segmentation\n",
        "        self.process_segmentation(input_path, output_dir, seq_file_path)\n",
        "\n",
        "        return output_dir, seq_file_path, pose_file_path if self.cache_pose and is_vid else None\n",
        "\n",
        "\n",
        "class VideoProcessCallable(VideoProcessBase):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(VideoProcessCallable, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, input_path, output_dir=None):\n",
        "        return self.cache(input_path, output_dir)\n",
        "\n",
        "\n",
        "def smooth_poses(poses, kernel_size=5):\n",
        "    out_poses = poses.copy() if isinstance(poses, np.ndarray) else np.array(poses)\n",
        "\n",
        "    # Prepare smoothing kernel\n",
        "    # w = np.hamming(kernel_size)\n",
        "    w = np.ones(kernel_size)\n",
        "    w /= w.sum()\n",
        "\n",
        "    # Smooth poses\n",
        "    poses_padded = np.pad(out_poses, ((kernel_size // 2, kernel_size // 2), (0, 0)), 'reflect')\n",
        "    for i in range(out_poses.shape[1]):\n",
        "        out_poses[:, i] = np.convolve(w, poses_padded[:, i], mode='valid')\n",
        "\n",
        "    return out_poses\n",
        "\n",
        "\n",
        "def main(input, output=d('output'), resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth')):\n",
        "    video_process = VideoProcessCallable(\n",
        "        resolution, crop_scale, gpus, cpu_only, display, verbose, encoder_codec,\n",
        "        detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "        iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "        size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "        pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix, cache_pose=cache_pose,\n",
        "        cache_frontal=cache_frontal, smooth_poses=smooth_poses, lms_model=lms_model, lms_batch_size=lms_batch_size,\n",
        "        landmarks_postfix=landmarks_postfix, cache_landmarks=cache_landmarks, smooth_landmarks=smooth_landmarks,\n",
        "        seg_model=seg_model, seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "        cache_segmentation=cache_segmentation, smooth_segmentation=smooth_segmentation,\n",
        "        seg_remove_mouth=seg_remove_mouth)\n",
        "    if len(input) == 1 and os.path.isfile(input[0]):\n",
        "        video_process.cache(input, output)\n",
        "    else:\n",
        "        batch(input, None, output, video_process, postfix='.mp4')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(**vars(parser.parse_args()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting fsgan/preprocess/preprocess_video.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbcZplL8uH1d",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c0bf3da1-f901-4c77-8c66-2a8d1158230b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title 3，使用 mp4v 需要修改 fsgan/preprocess/crop_video_sequences.py\n",
        "%%writefile fsgan/preprocess/crop_video_sequences.py\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from fsgan.utils.bbox_utils import scale_bbox, crop_img\n",
        "from fsgan.utils.video_utils import Sequence\n",
        "\n",
        "\n",
        "def main(input_path, output_dir=None, cache_path=None, seq_postfix='_dsfd_seq.pkl', resolution=256, crop_scale=2.0,\n",
        "         select='all', disable_tqdm=False, encoder_codec='mp4v'):\n",
        "    cache_path = os.path.splitext(input_path)[0] + seq_postfix if cache_path is None else cache_path\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.splitext(input_path)[0]\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "\n",
        "    # Verification\n",
        "    if not os.path.isfile(input_path):\n",
        "        raise RuntimeError('Input video does not exist: ' + input_path)\n",
        "    if not os.path.isfile(cache_path):\n",
        "        raise RuntimeError('Cache file does not exist: ' + cache_path)\n",
        "    if not os.path.isdir(output_dir):\n",
        "        raise RuntimeError('Output directory does not exist: ' + output_dir)\n",
        "\n",
        "    print('=> Cropping video sequences from video: \"%s\"...' % os.path.basename(input_path))\n",
        "\n",
        "    # Load sequences from file\n",
        "    with open(cache_path, \"rb\") as fp:  # Unpickling\n",
        "        seq_list = pickle.load(fp)\n",
        "\n",
        "    # Select sequences\n",
        "    if select == 'longest':\n",
        "        selected_seq_index = np.argmax([len(s) for s in seq_list])\n",
        "        seq = seq_list[selected_seq_index]\n",
        "        seq.id = 0\n",
        "        seq_list = [seq]\n",
        "\n",
        "    # Open input video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError('Failed to read video: ' + input_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    input_vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    input_vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # For each sequence initialize output video file\n",
        "    out_vids = []\n",
        "    fourcc = cv2.VideoWriter_fourcc(*encoder_codec)\n",
        "    for seq in seq_list:\n",
        "        curr_vid_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d.mp4' % seq.id\n",
        "        curr_vid_path = os.path.join(output_dir, curr_vid_name)\n",
        "        out_vids.append(cv2.VideoWriter(curr_vid_path, fourcc, fps, (resolution, resolution)))\n",
        "\n",
        "    # For each frame in the target video\n",
        "    cropped_detections = [[] for seq in seq_list]\n",
        "    cropped_landmarks = [[] for seq in seq_list]\n",
        "    pbar = range(total_frames) if disable_tqdm else tqdm(range(total_frames), file=sys.stdout)\n",
        "    for i in pbar:\n",
        "        ret, frame = cap.read()\n",
        "        if frame is None:\n",
        "            continue\n",
        "\n",
        "        # For each sequence\n",
        "        for s, seq in enumerate(seq_list):\n",
        "            if i < seq.start_index or (seq.start_index + len(seq) - 1) < i:\n",
        "                continue\n",
        "            det = seq[i - seq.start_index]\n",
        "\n",
        "            # Crop frame\n",
        "            bbox = np.concatenate((det[:2], det[2:] - det[:2]))\n",
        "            bbox = scale_bbox(bbox, crop_scale)\n",
        "            frame_cropped = crop_img(frame, bbox)\n",
        "            frame_cropped = cv2.resize(frame_cropped, (resolution, resolution), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Write cropped frame to output video\n",
        "            out_vids[s].write(frame_cropped)\n",
        "\n",
        "            # Add cropped detection to list\n",
        "            orig_size = bbox[2:]\n",
        "            axes_scale = np.array([resolution, resolution]) / orig_size\n",
        "            det[:2] -= bbox[:2]\n",
        "            det[2:] -= bbox[:2]\n",
        "            det[:2] *= axes_scale\n",
        "            det[2:] *= axes_scale\n",
        "            cropped_detections[s].append(det)\n",
        "\n",
        "            # Add cropped landmarks to list\n",
        "            if hasattr(seq, 'landmarks'):\n",
        "                curr_landmarks = seq.landmarks[i - seq.start_index]\n",
        "                curr_landmarks[:, :2] -= bbox[:2]\n",
        "\n",
        "                # 3D landmarks case\n",
        "                if curr_landmarks.shape[1] == 3:\n",
        "                    axes_scale = np.append(axes_scale, axes_scale.mean())\n",
        "\n",
        "                curr_landmarks *= axes_scale\n",
        "                cropped_landmarks[s].append(curr_landmarks)\n",
        "\n",
        "    # For each sequence write cropped sequence to file\n",
        "    for s, seq in enumerate(seq_list):\n",
        "        # seq.detections = np.array(cropped_detections[s])\n",
        "        # if hasattr(seq, 'landmarks'):\n",
        "        #     seq.landmarks = np.array(cropped_landmarks[s])\n",
        "        # seq.start_index = 0\n",
        "\n",
        "        # TODO: this is a hack to change class type (remove this later)\n",
        "        out_seq = Sequence(0)\n",
        "        out_seq.detections = np.array(cropped_detections[s])\n",
        "        if hasattr(seq, 'landmarks'):\n",
        "            out_seq.landmarks = np.array(cropped_landmarks[s])\n",
        "        out_seq.id, out_seq.obj_id, out_seq.size_avg = seq.id, seq.obj_id, seq.size_avg\n",
        "\n",
        "        # Write to file\n",
        "        curr_out_name = os.path.splitext(os.path.basename(input_path))[0] + '_seq%02d%s' % (out_seq.id, seq_postfix)\n",
        "        curr_out_path = os.path.join(output_dir, curr_out_name)\n",
        "        with open(curr_out_path, \"wb\") as fp:  # Pickling\n",
        "            pickle.dump([out_seq], fp)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parse program arguments\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser('crop_video_sequences')\n",
        "    parser.add_argument('input', metavar='VIDEO',\n",
        "                        help='path to input video')\n",
        "    parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                        help='output directory')\n",
        "    parser.add_argument('-c', '--cache', metavar='PATH',\n",
        "                        help='path to sequence cache file')\n",
        "    parser.add_argument('-sp', '--seq_postfix', default='_dsfd_seq.pkl', metavar='POSTFIX',\n",
        "                        help='input sequence file postfix')\n",
        "    parser.add_argument('-r', '--resolution', default=256, type=int, metavar='N',\n",
        "                        help='output video resolution (default: 256)')\n",
        "    parser.add_argument('-cs', '--crop_scale', default=2.0, type=float, metavar='F',\n",
        "                        help='crop scale relative to bounding box (default: 2.0)')\n",
        "    parser.add_argument('-s', '--select', default='all', metavar='STR',\n",
        "                        help='selection method [all|longest]')\n",
        "    parser.add_argument('-dt', '--disable_tqdm', dest='disable_tqdm', action='store_true',\n",
        "                          help='if specified disables tqdm progress bar')\n",
        "    parser.add_argument('-ec', '--encoder_codec', default='mp4v', metavar='STR',\n",
        "                        help='encoder codec code')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input, args.output, args.cache, args.seq_postfix, args.resolution, args.crop_scale, args.select,\n",
        "         args.disable_tqdm, args.encoder_codec)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting fsgan/preprocess/crop_video_sequences.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsXx7hZvP4_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "form",
        "outputId": "02a19bb7-8ba9-4244-e3f0-39ea839e1fcf"
      },
      "source": [
        "#@title 4， 使用 mp4v 需要修改 fshome/swap.py\n",
        "%%writefile swap.py\n",
        "\"\"\" Face swapping inference pipeline.\n",
        "\n",
        "This script implements face swapping for both images and videos using an appearance map for the source subject.\n",
        "The main pipeline components are: face reenactment and segmentation, inpainting, and blending.\n",
        "\n",
        "Information about both source and target files will be extracted and cached in directories by the file's name without\n",
        "the extension, residing in the same directory as the file. The information contains: face detections, face sequences,\n",
        "and cropped videos per sequence. In addition for each cropped video, the corresponding pose, landmarks, and\n",
        "segmentation masks will be computed and cached.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import RandomSampler, DataLoader\n",
        "from fsgan.preprocess.preprocess_video import VideoProcessBase, base_parser\n",
        "from fsgan.utils.obj_factory import obj_factory\n",
        "from fsgan.utils.utils import load_model\n",
        "from fsgan.utils.img_utils import bgr2tensor, tensor2bgr, create_pyramid\n",
        "from fsgan.utils.landmarks_utils import LandmarksHeatMapDecoder\n",
        "from fsgan.utils.seg_utils import blend_seg_label, SoftErosion\n",
        "from fsgan.datasets.img_lms_pose_transforms import RandomHorizontalFlip, Rotate, Pyramids, ToTensor, Normalize\n",
        "from fsgan.datasets import img_lms_pose_transforms\n",
        "from fsgan.datasets.seq_dataset import SeqInferenceDataset, SingleSeqRandomPairDataset\n",
        "from fsgan.datasets.appearance_map import AppearanceMapDataset\n",
        "from fsgan.utils.video_renderer import VideoRenderer\n",
        "from fsgan.utils.batch import main as batch\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "                                 parents=[base_parser])\n",
        "parser.add_argument('source', metavar='SOURCE', nargs='+',\n",
        "                    help='image or video per source: files, directories, file lists or queries')\n",
        "parser.add_argument('-t', '--target', metavar='TARGET', nargs='+',\n",
        "                    help='video per target: files, directories, file lists or queries')\n",
        "parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                    help='output directory')\n",
        "parser.add_argument('-ss', '--select_source', default='longest', metavar='STR',\n",
        "                    help='source selection method [\"longest\" | sequence number]')\n",
        "parser.add_argument('-st', '--select_target', default='longest', metavar='STR',\n",
        "                    help='target selection method [\"longest\" | sequence number]')\n",
        "parser.add_argument('-b', '--batch_size', default=8, type=int, metavar='N',\n",
        "                    help='mini-batch size')\n",
        "parser.add_argument('-rm', '--reenactment_model', metavar='PATH',\n",
        "                    default='../weights/nfv_msrunet_256_1_2_reenactment_v2.1.pth', help='reenactment model')\n",
        "parser.add_argument('-cm', '--completion_model', default='../weights/ijbc_msrunet_256_1_2_inpainting_v2.pth',\n",
        "                    metavar='PATH', help='completion model')\n",
        "parser.add_argument('-bm', '--blending_model', default='../weights/ijbc_msrunet_256_1_2_blending_v2.pth',\n",
        "                    metavar='PATH', help='blending model')\n",
        "parser.add_argument('-ci', '--criterion_id', default=\"vgg_loss.VGGLoss('../weights/vggface2_vgg19_256_1_2_id.pth')\",\n",
        "                    metavar='OBJ', help='id criterion object')\n",
        "parser.add_argument('-mr', '--min_radius', default=2.0, type=float, metavar='F',\n",
        "                    help='minimum distance between points in the appearance map')\n",
        "parser.add_argument('-oc', '--output_crop', action='store_true',\n",
        "                    help='output crop around the face instead of full frame')\n",
        "\n",
        "finetune = parser.add_argument_group('finetune')\n",
        "finetune.add_argument('-f', '--finetune', action='store_true',\n",
        "                      help='Toggle whether to finetune the reenactment generator (default: False)')\n",
        "finetune.add_argument('-fi', '--finetune_iterations', default=800, type=int, metavar='N',\n",
        "                      help='number of finetune iterations')\n",
        "finetune.add_argument('-fl', '--finetune_lr', default=1e-4, type=float, metavar='F',\n",
        "                      help='finetune learning rate')\n",
        "finetune.add_argument('-fb', '--finetune_batch_size', default=4, type=int, metavar='N',\n",
        "                      help='finetune batch size')\n",
        "finetune.add_argument('-fw', '--finetune_workers', default=4, type=int, metavar='N',\n",
        "                      help='finetune workers')\n",
        "finetune.add_argument('-fs', '--finetune_save', action='store_true',\n",
        "                      help='enable saving finetune checkpoint')\n",
        "d = parser.get_default\n",
        "\n",
        "\n",
        "class FaceSwapping(VideoProcessBase):\n",
        "    def __init__(self, resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "        cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "        # Detection arguments:\n",
        "        detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "        # Sequence arguments:\n",
        "        iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "        center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "        seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "        # Pose arguments:\n",
        "        pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "        cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "        # Landmarks arguments:\n",
        "        lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "        cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "        # Segmentation arguments:\n",
        "        seg_model=d('seg_model'), smooth_segmentation=d('smooth_segmentation'),\n",
        "        segmentation_postfix=d('segmentation_postfix'), cache_segmentation=d('cache_segmentation'),\n",
        "        seg_batch_size=d('seg_batch_size'), seg_remove_mouth=d('seg_remove_mouth'),\n",
        "        # Finetune arguments:\n",
        "        finetune=d('finetune'), finetune_iterations=d('finetune_iterations'), finetune_lr=d('finetune_lr'),\n",
        "        finetune_batch_size=d('finetune_batch_size'), finetune_workers=d('finetune_workers'),\n",
        "        finetune_save=d('finetune_save'),\n",
        "        # Swapping arguments:\n",
        "        batch_size=d('batch_size'), reenactment_model=d('reenactment_model'), completion_model=d('completion_model'),\n",
        "        blending_model=d('blending_model'), criterion_id=d('criterion_id'), min_radius=d('min_radius'),\n",
        "        output_crop=d('output_crop')):\n",
        "        super(FaceSwapping, self).__init__(\n",
        "            resolution, crop_scale, gpus, cpu_only, display, verbose, encoder_codec,\n",
        "            detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "            iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "            size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "            pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix,\n",
        "            cache_pose=True, cache_frontal=cache_frontal, smooth_poses=smooth_poses,\n",
        "            lms_model=lms_model, lms_batch_size=lms_batch_size, landmarks_postfix=landmarks_postfix,\n",
        "            cache_landmarks=True, smooth_landmarks=smooth_landmarks, seg_model=seg_model,\n",
        "            seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "            cache_segmentation=True, smooth_segmentation=smooth_segmentation, seg_remove_mouth=seg_remove_mouth)\n",
        "        self.batch_size = batch_size\n",
        "        self.min_radius = min_radius\n",
        "        self.output_crop = output_crop\n",
        "        self.finetune_enabled = finetune\n",
        "        self.finetune_iterations = finetune_iterations\n",
        "        self.finetune_lr = finetune_lr\n",
        "        self.finetune_batch_size = finetune_batch_size\n",
        "        self.finetune_workers = finetune_workers\n",
        "        self.finetune_save = finetune_save\n",
        "\n",
        "        # Load reenactment model\n",
        "        self.Gr, checkpoint = load_model(reenactment_model, 'face reenactment', self.device, return_checkpoint=True)\n",
        "        self.Gr.arch = checkpoint['arch']\n",
        "        self.reenactment_state_dict = checkpoint['state_dict']\n",
        "\n",
        "        # Load all other models\n",
        "        self.Gc = load_model(completion_model, 'face completion', self.device)\n",
        "        self.Gb = load_model(blending_model, 'face blending', self.device)\n",
        "\n",
        "        # Initialize landmarks decoders\n",
        "        self.landmarks_decoders = []\n",
        "        for res in (128, 256):\n",
        "            self.landmarks_decoders.insert(0, LandmarksHeatMapDecoder(res).to(self.device))\n",
        "\n",
        "        # Initialize losses\n",
        "        self.criterion_pixelwise = nn.L1Loss().to(self.device)\n",
        "        self.criterion_id = obj_factory(criterion_id).to(self.device)\n",
        "\n",
        "        # Support multiple GPUs\n",
        "        if self.gpus and len(self.gpus) > 1:\n",
        "            self.Gr = nn.DataParallel(self.Gr, self.gpus)\n",
        "            self.Gc = nn.DataParallel(self.Gc, self.gpus)\n",
        "            self.Gb = nn.DataParallel(self.Gb, self.gpus)\n",
        "            self.criterion_id.vgg = nn.DataParallel(self.criterion_id.vgg, self.gpus)\n",
        "\n",
        "        # Initialize soft erosion\n",
        "        self.smooth_mask = SoftErosion(kernel_size=21, threshold=0.6).to(self.device)\n",
        "\n",
        "        # Initialize video writer\n",
        "        self.video_renderer = FaceSwappingRenderer(self.display, self.verbose, self.output_crop, self.resolution,\n",
        "                                                   self.crop_scale, encoder_codec)\n",
        "        self.video_renderer.start()\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'video_renderer'):\n",
        "            self.video_renderer.kill()\n",
        "\n",
        "    def finetune(self, source_path, save_checkpoint=True):\n",
        "        checkpoint_path = os.path.splitext(source_path)[0] + '_Gr.pth'\n",
        "        if os.path.isfile(checkpoint_path):\n",
        "            print('=> Loading the reenactment generator finetuned on: \"%s\"...' % os.path.basename(source_path))\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            if self.gpus and len(self.gpus) > 1:\n",
        "                self.Gr.module.load_state_dict(checkpoint['state_dict'])\n",
        "            else:\n",
        "                self.Gr.load_state_dict(checkpoint['state_dict'])\n",
        "            return\n",
        "\n",
        "        print('=> Finetuning the reenactment generator on: \"%s\"...' % os.path.basename(source_path))\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.Gr.train(True)\n",
        "        img_transforms = img_lms_pose_transforms.Compose([Pyramids(2), ToTensor(), Normalize()])\n",
        "        train_dataset = SingleSeqRandomPairDataset(source_path, transform=img_transforms, postfixes=('_lms.npz',))\n",
        "        train_sampler = RandomSampler(train_dataset, replacement=True, num_samples=self.finetune_iterations)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.finetune_batch_size, sampler=train_sampler,\n",
        "                                  num_workers=self.finetune_workers, pin_memory=True, drop_last=True, shuffle=False)\n",
        "        optimizer = optim.Adam(self.Gr.parameters(), lr=self.finetune_lr, betas=(0.5, 0.999))\n",
        "\n",
        "        # For each batch in the training data\n",
        "        for i, (img, landmarks) in enumerate(tqdm(train_loader, unit='batches', file=sys.stdout)):\n",
        "            # Prepare input\n",
        "            with torch.no_grad():\n",
        "                # For each view images and landmarks\n",
        "                landmarks[1] = landmarks[1].to(self.device)\n",
        "                for j in range(len(img)):\n",
        "                    # For each pyramid image: push to device\n",
        "                    for p in range(len(img[j])):\n",
        "                        img[j][p] = img[j][p].to(self.device)\n",
        "\n",
        "                # Concatenate pyramid images with context to derive the final input\n",
        "                input = []\n",
        "                for p in range(len(img[0])):\n",
        "                    context = self.landmarks_decoders[p](landmarks[1])\n",
        "                    input.append(torch.cat((img[0][p], context), dim=1))\n",
        "\n",
        "            # Reenactment\n",
        "            img_pred = self.Gr(input)\n",
        "\n",
        "            # Reconstruction loss\n",
        "            loss_pixelwise = self.criterion_pixelwise(img_pred, img[1][0])\n",
        "            loss_id = self.criterion_id(img_pred, img[1][0])\n",
        "            loss_rec = 0.1 * loss_pixelwise + loss_id\n",
        "\n",
        "            # Update generator weights\n",
        "            optimizer.zero_grad()\n",
        "            loss_rec.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Save finetuned weights to file\n",
        "        if save_checkpoint:\n",
        "            arch = self.Gr.module.arch if self.gpus and len(self.gpus) > 1 else self.Gr.arch\n",
        "            state_dict = self.Gr.module.state_dict() if self.gpus and len(self.gpus) > 1 else self.Gr.state_dict()\n",
        "            torch.save({'state_dict': state_dict, 'arch': arch}, checkpoint_path)\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        self.Gr.train(False)\n",
        "\n",
        "    def __call__(self, source_path, target_path, output_path=None, select_source='longest', select_target='longest',\n",
        "                 finetune=None):\n",
        "        is_vid = os.path.splitext(source_path)[1] == '.mp4'\n",
        "        finetune = self.finetune_enabled and is_vid if finetune is None else finetune and is_vid\n",
        "\n",
        "        # Validation\n",
        "        assert os.path.isfile(source_path), 'Source path \"%s\" does not exist' % source_path\n",
        "        assert os.path.isfile(target_path), 'Target path \"%s\" does not exist' % target_path\n",
        "\n",
        "        # Cache input\n",
        "        source_cache_dir, source_seq_file_path, _ = self.cache(source_path)\n",
        "        target_cache_dir, target_seq_file_path, _ = self.cache(target_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(source_seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            source_seq_list = pickle.load(fp)\n",
        "        with open(target_seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            target_seq_list = pickle.load(fp)\n",
        "\n",
        "        # Select source and target sequence\n",
        "        source_seq = select_seq(source_seq_list, select_source)\n",
        "        target_seq = select_seq(target_seq_list, select_target)\n",
        "\n",
        "        # Set source and target sequence videos paths\n",
        "        src_path_no_ext, src_ext = os.path.splitext(source_path)\n",
        "        src_vid_seq_name = os.path.basename(src_path_no_ext) + '_seq%02d%s' % (source_seq.id, src_ext)\n",
        "        src_vid_seq_path = os.path.join(source_cache_dir, src_vid_seq_name)\n",
        "        tgt_path_no_ext, tgt_ext = os.path.splitext(target_path)\n",
        "        tgt_vid_seq_name = os.path.basename(tgt_path_no_ext) + '_seq%02d%s' % (target_seq.id, tgt_ext)\n",
        "        tgt_vid_seq_path = os.path.join(target_cache_dir, tgt_vid_seq_name)\n",
        "\n",
        "        # Set output path\n",
        "        if output_path is not None:\n",
        "            if os.path.isdir(output_path):\n",
        "                output_filename = f'{os.path.basename(src_path_no_ext)}_{os.path.basename(tgt_path_no_ext)}.mp4'\n",
        "                output_path = os.path.join(output_path, output_filename)\n",
        "\n",
        "        # Initialize appearance map\n",
        "        src_transform = img_lms_pose_transforms.Compose([Rotate(), Pyramids(2), ToTensor(), Normalize()])\n",
        "        tgt_transform = img_lms_pose_transforms.Compose([ToTensor(), Normalize()])\n",
        "        appearance_map = AppearanceMapDataset(src_vid_seq_path, tgt_vid_seq_path, src_transform, tgt_transform,\n",
        "                                              self.landmarks_postfix, self.pose_postfix, self.segmentation_postfix,\n",
        "                                              self.min_radius)\n",
        "        appearance_map_loader = DataLoader(appearance_map, batch_size=self.batch_size, num_workers=1, pin_memory=True,\n",
        "                                           drop_last=False, shuffle=False)\n",
        "\n",
        "        # Initialize video writer\n",
        "        self.video_renderer.init(target_path, target_seq, output_path, _appearance_map=appearance_map)\n",
        "\n",
        "        # Finetune reenactment model on source sequences\n",
        "        if finetune:\n",
        "            self.finetune(src_vid_seq_path, self.finetune_save)\n",
        "\n",
        "        print(f'=> Face swapping: \"{src_vid_seq_name}\" -> \"{tgt_vid_seq_name}\"...')\n",
        "\n",
        "        # For each batch of frames in the target video\n",
        "        for i, (src_frame, src_landmarks, src_poses, bw, tgt_frame, tgt_landmarks, tgt_pose, tgt_mask) \\\n",
        "                in enumerate(tqdm(appearance_map_loader, unit='batches', file=sys.stdout)):\n",
        "            # Prepare input\n",
        "            for p in range(len(src_frame)):\n",
        "                src_frame[p] = src_frame[p].to(self.device)\n",
        "            tgt_frame = tgt_frame.to(self.device)\n",
        "            tgt_landmarks = tgt_landmarks.to(self.device)\n",
        "            # tgt_mask = tgt_mask.unsqueeze(1).to(self.device)\n",
        "            tgt_mask = tgt_mask.unsqueeze(1).int().to(self.device).bool()   # TODO: check if the boolean tensor bug is fixed\n",
        "            bw = bw.to(self.device)\n",
        "            bw_indices = torch.nonzero(torch.any(bw > 0, dim=0), as_tuple=True)[0]\n",
        "            bw = bw[:, bw_indices]\n",
        "\n",
        "            # For each source frame perform reenactment\n",
        "            reenactment_triplet = []\n",
        "            for j in bw_indices:\n",
        "                input = []\n",
        "                for p in range(len(src_frame)):\n",
        "                    context = self.landmarks_decoders[p](tgt_landmarks)\n",
        "                    input.append(torch.cat((src_frame[p][:, j], context), dim=1))\n",
        "\n",
        "                # Reenactment\n",
        "                reenactment_triplet.append(self.Gr(input).unsqueeze(1))\n",
        "            reenactment_tensor = torch.cat(reenactment_triplet, dim=1)\n",
        "\n",
        "            # Barycentric interpolation of reenacted frames\n",
        "            reenactment_tensor = (reenactment_tensor * bw.view(*bw.shape, 1, 1, 1)).sum(dim=1)\n",
        "\n",
        "            # Compute reenactment segmentation\n",
        "            reenactment_seg = self.S(reenactment_tensor)\n",
        "            reenactment_background_mask_tensor = (reenactment_seg.argmax(1) != 1).unsqueeze(1)\n",
        "\n",
        "            # Remove the background of the aligned face\n",
        "            reenactment_tensor.masked_fill_(reenactment_background_mask_tensor, -1.0)\n",
        "\n",
        "            # Soften target mask\n",
        "            soft_tgt_mask, eroded_tgt_mask = self.smooth_mask(tgt_mask)\n",
        "\n",
        "            # Complete face\n",
        "            inpainting_input_tensor = torch.cat((reenactment_tensor, eroded_tgt_mask.float()), dim=1)\n",
        "            inpainting_input_tensor_pyd = create_pyramid(inpainting_input_tensor, 2)\n",
        "            completion_tensor = self.Gc(inpainting_input_tensor_pyd)\n",
        "\n",
        "            # Blend faces\n",
        "            transfer_tensor = transfer_mask(completion_tensor, tgt_frame, eroded_tgt_mask)\n",
        "            blend_input_tensor = torch.cat((transfer_tensor, tgt_frame, eroded_tgt_mask.float()), dim=1)\n",
        "            blend_input_tensor_pyd = create_pyramid(blend_input_tensor, 2)\n",
        "            blend_tensor = self.Gb(blend_input_tensor_pyd)\n",
        "\n",
        "            # Final result\n",
        "            result_tensor = blend_tensor * soft_tgt_mask + tgt_frame * (1 - soft_tgt_mask)\n",
        "\n",
        "            # Write output\n",
        "            if self.verbose == 0:\n",
        "                self.video_renderer.write(result_tensor)\n",
        "            elif self.verbose == 1:\n",
        "                curr_src_frames = [src_frame[0][:, i] for i in range(src_frame[0].shape[1])]\n",
        "                self.video_renderer.write(*curr_src_frames, result_tensor, tgt_frame)\n",
        "            else:\n",
        "                curr_src_frames = [src_frame[0][:, i] for i in range(src_frame[0].shape[1])]\n",
        "                tgt_seg_blend = blend_seg_label(tgt_frame, tgt_mask.squeeze(1))\n",
        "                soft_tgt_mask = soft_tgt_mask.mul(2.).sub(1.).repeat(1, 3, 1, 1)\n",
        "                self.video_renderer.write(*curr_src_frames, result_tensor, tgt_frame, reenactment_tensor,\n",
        "                                          completion_tensor, transfer_tensor, soft_tgt_mask, tgt_seg_blend,\n",
        "                                          tgt_pose)\n",
        "\n",
        "        # Load original reenactment weights\n",
        "        if finetune:\n",
        "            if self.gpus and len(self.gpus) > 1:\n",
        "                self.Gr.module.load_state_dict(self.reenactment_state_dict)\n",
        "            else:\n",
        "                self.Gr.load_state_dict(self.reenactment_state_dict)\n",
        "\n",
        "        # Wait for the video writer to finish writing\n",
        "        self.video_renderer.wait_until_finished()\n",
        "\n",
        "\n",
        "class FaceSwappingRenderer(VideoRenderer):\n",
        "    def __init__(self, display=False, verbose=0, output_crop=False, resolution=256, crop_scale=1.2,\n",
        "                 encoder_codec='mp4v'):\n",
        "        self._appearance_map = None\n",
        "        self._fig = None\n",
        "        self._figsize = (24, 16)\n",
        "\n",
        "        # Calculate verbose size\n",
        "        verbose_size, self._appearance_map_size = None, None\n",
        "        if verbose == 1:\n",
        "            verbose_size = (resolution * 5, resolution)\n",
        "        elif verbose >= 2:\n",
        "            fig_ratio = self._figsize[0] / self._figsize[1]\n",
        "            height = 5 * resolution\n",
        "            self._appearance_map_size = (int(np.round(height * fig_ratio)), height)\n",
        "            verbose_size = (self._appearance_map_size[0] + resolution * 2, self._appearance_map_size[1])\n",
        "\n",
        "        super(FaceSwappingRenderer, self).__init__(display, verbose, verbose_size, output_crop, resolution, crop_scale,\n",
        "                                                   encoder_codec)\n",
        "\n",
        "    def on_render(self, *args):\n",
        "        if self._verbose <= 0:\n",
        "            return tensor2bgr(args[0])\n",
        "        elif self._verbose == 1:\n",
        "            return tensor2bgr(torch.cat(args, dim=2))\n",
        "        else:\n",
        "            if self._fig is None:\n",
        "                self._fig = plt.figure(figsize=self._figsize)\n",
        "            results_bgr1 = tensor2bgr(torch.cat(args[:5], dim=1))\n",
        "            results_bgr2 = tensor2bgr(torch.cat(args[5:10], dim=1))\n",
        "            tgt_pose = args[-1].numpy()\n",
        "            appearance_map_bgr = render_appearance_map(self._fig, self._appearance_map.tri, self._appearance_map.points,\n",
        "                                                       tgt_pose[:2])\n",
        "            appearance_map_bgr = cv2.resize(appearance_map_bgr, self._appearance_map_size,\n",
        "                                            interpolation=cv2.INTER_CUBIC)\n",
        "            render_bgr = np.concatenate((appearance_map_bgr, results_bgr1, results_bgr2), axis=1)\n",
        "            tgt_pose *= 99.     # Unnormalize the target pose for printing\n",
        "            msg = 'Pose: %.1f, %.1f, %.1f' % (tgt_pose[0], tgt_pose[1], tgt_pose[2])\n",
        "            cv2.putText(render_bgr, msg, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            return render_bgr\n",
        "\n",
        "\n",
        "def render_appearance_map(fig, tri, points, query_point=None, render_scale=99.):\n",
        "    points_scaled = points * render_scale\n",
        "    plt.triplot(points_scaled[:, 0], points_scaled[:, 1], tri.simplices.copy(), linewidth=3)\n",
        "    plt.plot(points_scaled[:, 0], points_scaled[:, 1], 'o', markersize=12)\n",
        "    if query_point is not None:\n",
        "        query_point_scaled = query_point[:2] * render_scale\n",
        "        tri_index = tri.find_simplex(query_point[:2])\n",
        "        tri_vertices = tri.simplices[tri_index]\n",
        "        plt.plot(points_scaled[tri_vertices, 0], points_scaled[tri_vertices, 1], 'yo', markersize=12)\n",
        "        plt.plot(query_point_scaled[0], query_point_scaled[1], 'rx', markersize=24, markeredgewidth=4)\n",
        "\n",
        "    plt.xlim(points_scaled[:-4, 0].min() - 0.5, points_scaled[:-4, 0].max() + 0.5)\n",
        "    plt.ylim(points_scaled[:-4, 1].min() - 0.5, points_scaled[:-4, 1].max() + 0.5)\n",
        "    plt.xlabel('Yaw (angles)', fontsize=24)\n",
        "    plt.ylabel('Pitch (angles)', fontsize=24)\n",
        "    ax = plt.gca()\n",
        "    ax.tick_params(axis='both', which='major', labelsize=24)\n",
        "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    # grab the pixel buffer and dump it into a numpy array\n",
        "    img = np.array(fig.canvas.renderer._renderer)\n",
        "    plt.clf()\n",
        "\n",
        "    return img[:, :, 2::-1]\n",
        "\n",
        "\n",
        "def select_seq(seq_list, select='longest'):\n",
        "    if select == 'longest':\n",
        "        seq = seq_list[np.argmax([len(s) for s in seq_list])]\n",
        "    elif select.isnumeric():\n",
        "        seq = seq_list[[s.id for s in seq_list].index(int(select))]\n",
        "    else:\n",
        "        raise RuntimeError(f'Unknown selection method: \"{select}\"')\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "def transfer_mask(img1, img2, mask):\n",
        "    mask = mask.repeat(1, 3, 1, 1).float()\n",
        "    out = img1 * mask + img2 * (1 - mask)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def main(source, target, output=None, select_source=d('select_source'), select_target=d('select_target'),\n",
        "         # General arguments\n",
        "         resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth'),\n",
        "         # Finetune arguments:\n",
        "         finetune=d('finetune'), finetune_iterations=d('finetune_iterations'), finetune_lr=d('finetune_lr'),\n",
        "         finetune_batch_size=d('finetune_batch_size'), finetune_workers=d('finetune_workers'),\n",
        "         finetune_save=d('finetune_save'),\n",
        "         # Swapping arguments:\n",
        "         batch_size=d('batch_size'), reenactment_model=d('reenactment_model'), completion_model=d('completion_model'),\n",
        "         blending_model=d('blending_model'), criterion_id=d('criterion_id'), min_radius=d('min_radius'),\n",
        "         output_crop=d('output_crop')):\n",
        "    face_swapping = FaceSwapping(\n",
        "        resolution, crop_scale, gpus, cpu_only, display, verbose, encoder_codec,\n",
        "        detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "        iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "        size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "        pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix,\n",
        "        cache_pose=cache_pose, cache_frontal=cache_frontal, smooth_poses=smooth_poses,\n",
        "        lms_model=lms_model, lms_batch_size=lms_batch_size, landmarks_postfix=landmarks_postfix,\n",
        "        cache_landmarks=cache_landmarks, smooth_landmarks=smooth_landmarks,\n",
        "        seg_model=seg_model, seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "        cache_segmentation=cache_segmentation, smooth_segmentation=smooth_segmentation,\n",
        "        seg_remove_mouth=seg_remove_mouth,\n",
        "        finetune=finetune, finetune_iterations=finetune_iterations, finetune_lr=finetune_lr,\n",
        "        finetune_batch_size=finetune_batch_size, finetune_workers=finetune_workers, finetune_save=finetune_save,\n",
        "        batch_size=batch_size, reenactment_model=reenactment_model, completion_model=completion_model,\n",
        "        blending_model=blending_model, criterion_id=criterion_id, min_radius=min_radius, output_crop=output_crop)\n",
        "    if len(source) == 1 and len(target) == 1 and os.path.isfile(source[0]) and os.path.isfile(target[0]):\n",
        "        face_swapping(source[0], target[0], output, select_source, select_target)\n",
        "    else:\n",
        "        batch(source, target, output, face_swapping, postfix='.mp4', skip_existing=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(**vars(parser.parse_args()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting swap.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vz1XVBPRbqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "form",
        "outputId": "3fd735b4-d18c-49ba-b988-95fa399293f6"
      },
      "source": [
        "#@title 5， 使用 mp4v 需要修改 fsgan/inference/reenact.py\n",
        "%%writefile fsgan/inference/reenact.py\n",
        "\"\"\" Face reenactment inference pipeline.\n",
        "\n",
        "This script implements face reenactment for both images and videos using an appearance map for the source subject.\n",
        "\n",
        "Information about both source and target files will be extracted and cached in directories by the file's name without\n",
        "the extension, residing in the same directory as the file. The information contains: face detections, face sequences,\n",
        "and cropped videos per sequence. In addition for each cropped video, the corresponding pose, landmarks, and\n",
        "segmentation masks will be computed and cached.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import RandomSampler, DataLoader\n",
        "from fsgan.preprocess.preprocess_video import VideoProcessBase, base_parser\n",
        "from fsgan.utils.obj_factory import obj_factory\n",
        "from fsgan.utils.utils import load_model\n",
        "from fsgan.utils.img_utils import tensor2bgr\n",
        "from fsgan.utils.landmarks_utils import LandmarksHeatMapDecoder\n",
        "from fsgan.datasets.img_lms_pose_transforms import RandomHorizontalFlip, Rotate, Pyramids, ToTensor, Normalize\n",
        "from fsgan.datasets import img_lms_pose_transforms\n",
        "from fsgan.datasets.seq_dataset import SingleSeqRandomPairDataset\n",
        "from fsgan.datasets.appearance_map import AppearanceMapDataset\n",
        "from fsgan.utils.video_renderer import VideoRenderer\n",
        "from fsgan.utils.batch import main as batch\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "                                 parents=[base_parser])\n",
        "parser.add_argument('source', metavar='SOURCE', nargs='+',\n",
        "                    help='image or video per source: files, directories, file lists or queries')\n",
        "parser.add_argument('-t', '--target', metavar='TARGET', nargs='+',\n",
        "                    help='video per target: files, directories, file lists or queries')\n",
        "parser.add_argument('-o', '--output', metavar='DIR',\n",
        "                    help='output directory')\n",
        "parser.add_argument('-ss', '--select_source', default='longest', metavar='STR',\n",
        "                    help='source selection method [\"longest\" | sequence number]')\n",
        "parser.add_argument('-st', '--select_target', default='longest', metavar='STR',\n",
        "                    help='target selection method [\"longest\" | sequence number]')\n",
        "parser.add_argument('-b', '--batch_size', default=8, type=int, metavar='N',\n",
        "                    help='mini-batch size')\n",
        "parser.add_argument('-rm', '--reenactment_model', metavar='PATH',\n",
        "                    default='../weights/nfv_msrunet_256_1_2_reenactment_v2.1.pth', help='reenactment model')\n",
        "parser.add_argument('-ci', '--criterion_id', default=\"vgg_loss.VGGLoss('../weights/vggface2_vgg19_256_1_2_id.pth')\",\n",
        "                    metavar='OBJ', help='id criterion object')\n",
        "parser.add_argument('-mr', '--min_radius', default=2.0, type=float, metavar='F',\n",
        "                    help='minimum distance between points in the appearance map')\n",
        "\n",
        "finetune = parser.add_argument_group('finetune')\n",
        "finetune.add_argument('-f', '--finetune', action='store_true',\n",
        "                      help='Toggle whether to finetune the reenactment generator (default: False)')\n",
        "finetune.add_argument('-fi', '--finetune_iterations', default=800, type=int, metavar='N',\n",
        "                      help='number of finetune iterations')\n",
        "finetune.add_argument('-fl', '--finetune_lr', default=1e-4, type=float, metavar='F',\n",
        "                      help='finetune learning rate')\n",
        "finetune.add_argument('-fb', '--finetune_batch_size', default=4, type=int, metavar='N',\n",
        "                      help='finetune batch size')\n",
        "finetune.add_argument('-fw', '--finetune_workers', default=4, type=int, metavar='N',\n",
        "                      help='finetune workers')\n",
        "finetune.add_argument('-fs', '--finetune_save', action='store_true',\n",
        "                      help='enable saving finetune checkpoint')\n",
        "d = parser.get_default\n",
        "\n",
        "\n",
        "class FaceReenactment(VideoProcessBase):\n",
        "    def __init__(self, resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "        cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "        # Detection arguments:\n",
        "        detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "        # Sequence arguments:\n",
        "        iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "        center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "        seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "        # Pose arguments:\n",
        "        pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "        cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "        # Landmarks arguments:\n",
        "        lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "        cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "        # Segmentation arguments:\n",
        "        seg_model=d('seg_model'), smooth_segmentation=d('smooth_segmentation'),\n",
        "        segmentation_postfix=d('segmentation_postfix'), cache_segmentation=d('cache_segmentation'),\n",
        "        seg_batch_size=d('seg_batch_size'), seg_remove_mouth=d('seg_remove_mouth'),\n",
        "        # Finetune arguments:\n",
        "        finetune=d('finetune'), finetune_iterations=d('finetune_iterations'), finetune_lr=d('finetune_lr'),\n",
        "        finetune_batch_size=d('finetune_batch_size'), finetune_workers=d('finetune_workers'),\n",
        "        finetune_save=d('finetune_save'),\n",
        "        # Reenactment arguments:\n",
        "        batch_size=d('batch_size'), reenactment_model=d('reenactment_model'), criterion_id=d('criterion_id'),\n",
        "        min_radius=d('min_radius')):\n",
        "        super(FaceReenactment, self).__init__(\n",
        "            resolution, crop_scale, gpus, cpu_only, display, verbose, encoder_codec,\n",
        "            detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "            iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "            size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "            pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix,\n",
        "            cache_pose=True, cache_frontal=cache_frontal, smooth_poses=smooth_poses,\n",
        "            lms_model=lms_model, lms_batch_size=lms_batch_size, landmarks_postfix=landmarks_postfix,\n",
        "            cache_landmarks=True, smooth_landmarks=smooth_landmarks, seg_model=seg_model,\n",
        "            seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "            cache_segmentation=True, smooth_segmentation=smooth_segmentation,\n",
        "            seg_remove_mouth=seg_remove_mouth)\n",
        "        self.batch_size = batch_size\n",
        "        self.min_radius = min_radius\n",
        "        self.finetune_enabled = finetune\n",
        "        self.finetune_iterations = finetune_iterations\n",
        "        self.finetune_lr = finetune_lr\n",
        "        self.finetune_batch_size = finetune_batch_size\n",
        "        self.finetune_workers = finetune_workers\n",
        "        self.finetune_save = finetune_save\n",
        "\n",
        "        # Load reenactment model\n",
        "        self.Gr, checkpoint = load_model(reenactment_model, 'face reenactment', self.device, return_checkpoint=True)\n",
        "        self.Gr.arch = checkpoint['arch']\n",
        "        self.reenactment_state_dict = checkpoint['state_dict']\n",
        "\n",
        "        # Initialize landmarks decoders\n",
        "        self.landmarks_decoders = []\n",
        "        for res in (128, 256):\n",
        "            self.landmarks_decoders.insert(0, LandmarksHeatMapDecoder(res).to(self.device))\n",
        "\n",
        "        # Initialize losses\n",
        "        self.criterion_pixelwise = nn.L1Loss().to(self.device)\n",
        "        self.criterion_id = obj_factory(criterion_id).to(self.device)\n",
        "\n",
        "        # Support multiple GPUs\n",
        "        if self.gpus and len(self.gpus) > 1:\n",
        "            self.Gr = nn.DataParallel(self.Gr, self.gpus)\n",
        "            self.criterion_id.vgg = nn.DataParallel(self.criterion_id.vgg, self.gpus)\n",
        "\n",
        "        # Initialize video renderer\n",
        "        self.video_renderer = FaceReenactmentRenderer(self.display, self.verbose, True, self.resolution,\n",
        "                                                      self.crop_scale, encoder_codec)\n",
        "        self.video_renderer.start()\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'video_renderer'):\n",
        "            self.video_renderer.kill()\n",
        "\n",
        "    def finetune(self, source_path, save_checkpoint=True):\n",
        "        checkpoint_path = os.path.splitext(source_path)[0] + '_Gr.pth'\n",
        "        if os.path.isfile(checkpoint_path):\n",
        "            print('=> Loading the reenactment generator finetuned on: \"%s\"...' % os.path.basename(source_path))\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            if self.gpus and len(self.gpus) > 1:\n",
        "                self.Gr.module.load_state_dict(checkpoint['state_dict'])\n",
        "            else:\n",
        "                self.Gr.load_state_dict(checkpoint['state_dict'])\n",
        "            return\n",
        "\n",
        "        print('=> Finetuning the reenactment generator on: \"%s\"...' % os.path.basename(source_path))\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.Gr.train(True)\n",
        "        img_transforms = img_lms_pose_transforms.Compose([Pyramids(2), ToTensor(), Normalize()])\n",
        "        train_dataset = SingleSeqRandomPairDataset(source_path, transform=img_transforms, postfixes=('_lms.npz',))\n",
        "        train_sampler = RandomSampler(train_dataset, replacement=True, num_samples=self.finetune_iterations)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.finetune_batch_size, sampler=train_sampler,\n",
        "                                  num_workers=self.finetune_workers, pin_memory=True, drop_last=True, shuffle=False)\n",
        "        optimizer = optim.Adam(self.Gr.parameters(), lr=self.finetune_lr, betas=(0.5, 0.999))\n",
        "\n",
        "        # For each batch in the training data\n",
        "        for i, (img, landmarks) in enumerate(tqdm(train_loader, unit='batches', file=sys.stdout)):\n",
        "            # Prepare input\n",
        "            with torch.no_grad():\n",
        "                # For each view images and landmarks\n",
        "                landmarks[1] = landmarks[1].to(self.device)\n",
        "                for j in range(len(img)):\n",
        "                    # For each pyramid image: push to device\n",
        "                    for p in range(len(img[j])):\n",
        "                        img[j][p] = img[j][p].to(self.device)\n",
        "\n",
        "                # Concatenate pyramid images with context to derive the final input\n",
        "                input = []\n",
        "                for p in range(len(img[0])):\n",
        "                    context = self.landmarks_decoders[p](landmarks[1])\n",
        "                    input.append(torch.cat((img[0][p], context), dim=1))\n",
        "\n",
        "            # Reenactment\n",
        "            img_pred = self.Gr(input)\n",
        "\n",
        "            # Reconstruction loss\n",
        "            loss_pixelwise = self.criterion_pixelwise(img_pred, img[1][0])\n",
        "            loss_id = self.criterion_id(img_pred, img[1][0])\n",
        "            loss_rec = 0.1 * loss_pixelwise + loss_id\n",
        "\n",
        "            # Update generator weights\n",
        "            optimizer.zero_grad()\n",
        "            loss_rec.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Save finetuned weights to file\n",
        "        if save_checkpoint:\n",
        "            arch = self.Gr.module.arch if self.gpus and len(self.gpus) > 1 else self.Gr.arch\n",
        "            state_dict = self.Gr.module.state_dict() if self.gpus and len(self.gpus) > 1 else self.Gr.state_dict()\n",
        "            torch.save({'state_dict': state_dict, 'arch': arch}, checkpoint_path)\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        self.Gr.train(False)\n",
        "\n",
        "    def __call__(self, source_path, target_path, output_path=None, select_source='longest', select_target='longest',\n",
        "                 finetune=None):\n",
        "        is_vid = os.path.splitext(source_path)[1] == '.mp4'\n",
        "        finetune = self.finetune_enabled and is_vid if finetune is None else finetune and is_vid\n",
        "\n",
        "        # Validation\n",
        "        assert os.path.isfile(source_path), 'Source path \"%s\" does not exist' % source_path\n",
        "        assert os.path.isfile(target_path), 'Target path \"%s\" does not exist' % target_path\n",
        "\n",
        "        # Cache input\n",
        "        source_cache_dir, source_seq_file_path, _ = self.cache(source_path)\n",
        "        target_cache_dir, target_seq_file_path, _ = self.cache(target_path)\n",
        "\n",
        "        # Load sequences from file\n",
        "        with open(source_seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            source_seq_list = pickle.load(fp)\n",
        "        with open(target_seq_file_path, \"rb\") as fp:  # Unpickling\n",
        "            target_seq_list = pickle.load(fp)\n",
        "\n",
        "        # Select source and target sequence\n",
        "        source_seq = select_seq(source_seq_list, select_source)\n",
        "        target_seq = select_seq(target_seq_list, select_target)\n",
        "\n",
        "        # Set source and target sequence videos paths\n",
        "        src_path_no_ext, src_ext = os.path.splitext(source_path)\n",
        "        src_vid_seq_name = os.path.basename(src_path_no_ext) + '_seq%02d%s' % (source_seq.id, src_ext)\n",
        "        src_vid_seq_path = os.path.join(source_cache_dir, src_vid_seq_name)\n",
        "        tgt_path_no_ext, tgt_ext = os.path.splitext(target_path)\n",
        "        tgt_vid_seq_name = os.path.basename(tgt_path_no_ext) + '_seq%02d%s' % (target_seq.id, tgt_ext)\n",
        "        tgt_vid_seq_path = os.path.join(target_cache_dir, tgt_vid_seq_name)\n",
        "\n",
        "        # Set output path\n",
        "        if output_path is not None:\n",
        "            if os.path.isdir(output_path):\n",
        "                output_filename = f'{os.path.basename(src_path_no_ext)}_{os.path.basename(tgt_path_no_ext)}.mp4'\n",
        "                output_path = os.path.join(output_path, output_filename)\n",
        "\n",
        "        # Initialize appearance map\n",
        "        src_transform = img_lms_pose_transforms.Compose([Rotate(), Pyramids(2), ToTensor(), Normalize()])\n",
        "        tgt_transform = img_lms_pose_transforms.Compose([ToTensor(), Normalize()])\n",
        "        appearance_map = AppearanceMapDataset(src_vid_seq_path, tgt_vid_seq_path, src_transform, tgt_transform,\n",
        "                                              self.landmarks_postfix, self.pose_postfix, self.segmentation_postfix,\n",
        "                                              self.min_radius)\n",
        "        appearance_map_loader = DataLoader(appearance_map, batch_size=self.batch_size, num_workers=1, pin_memory=True,\n",
        "                                           drop_last=False, shuffle=False)\n",
        "\n",
        "        # Initialize video renderer\n",
        "        self.video_renderer.init(target_path, target_seq, output_path, _appearance_map=appearance_map)\n",
        "\n",
        "        # Finetune reenactment model on source sequences\n",
        "        if finetune:\n",
        "            self.finetune(src_vid_seq_path, self.finetune_save)\n",
        "\n",
        "        print(f'=> Face reenactment: \"{src_vid_seq_name}\" -> \"{tgt_vid_seq_name}\"...')\n",
        "\n",
        "        # For each batch of frames in the target video\n",
        "        for i, (src_frame, src_landmarks, src_poses, bw, tgt_frame, tgt_landmarks, tgt_pose, tgt_mask) \\\n",
        "                in enumerate(tqdm(appearance_map_loader, unit='batches', file=sys.stdout)):\n",
        "            # Prepare input\n",
        "            for p in range(len(src_frame)):\n",
        "                src_frame[p] = src_frame[p].to(self.device)\n",
        "            tgt_landmarks = tgt_landmarks.to(self.device)\n",
        "            bw = bw.to(self.device)\n",
        "            bw_indices = torch.nonzero(torch.any(bw > 0, dim=0), as_tuple=True)[0]\n",
        "            bw = bw[:, bw_indices]\n",
        "\n",
        "            # For each source frame perform reenactment\n",
        "            reenactment_triplet = []\n",
        "            for j in bw_indices:\n",
        "                input = []\n",
        "                for p in range(len(src_frame)):\n",
        "                    context = self.landmarks_decoders[p](tgt_landmarks)\n",
        "                    input.append(torch.cat((src_frame[p][:, j], context), dim=1))\n",
        "\n",
        "                # Reenactment\n",
        "                reenactment_triplet.append(self.Gr(input).unsqueeze(1))\n",
        "            reenactment_tensor = torch.cat(reenactment_triplet, dim=1)\n",
        "\n",
        "            # Barycentric interpolation of reenacted frames\n",
        "            reenactment_tensor = (reenactment_tensor * bw.view(*bw.shape, 1, 1, 1)).sum(dim=1)\n",
        "\n",
        "            # Write output\n",
        "            if self.verbose == 0:\n",
        "                self.video_renderer.write(reenactment_tensor)\n",
        "            elif self.verbose == 1:\n",
        "                self.video_renderer.write(src_frame[0][:, 0], src_frame[0][:, 1], src_frame[0][:, 2],\n",
        "                                          reenactment_tensor, tgt_frame)\n",
        "            else:\n",
        "                self.video_renderer.write(src_frame[0][:, 0], src_frame[0][:, 1], src_frame[0][:, 2],\n",
        "                                          reenactment_tensor, tgt_frame, tgt_pose)\n",
        "\n",
        "        # Load original reenactment weights\n",
        "        if finetune:\n",
        "            if self.gpus and len(self.gpus) > 1:\n",
        "                self.Gr.module.load_state_dict(self.reenactment_state_dict)\n",
        "            else:\n",
        "                self.Gr.load_state_dict(self.reenactment_state_dict)\n",
        "\n",
        "        # Wait for the video render to finish rendering\n",
        "        self.video_renderer.wait_until_finished()\n",
        "\n",
        "\n",
        "class FaceReenactmentRenderer(VideoRenderer):\n",
        "    def __init__(self, display=False, verbose=0, output_crop=False, resolution=256, crop_scale=1.2,\n",
        "                 encoder_codec='mp4v'):\n",
        "        self._appearance_map = None\n",
        "        self._fig = None\n",
        "        self._figsize = (24, 16)\n",
        "\n",
        "        # Calculate verbose size\n",
        "        verbose_size, self._appearance_map_size = None, None\n",
        "        if verbose == 1:\n",
        "            verbose_size = (resolution * 5, resolution)\n",
        "        elif verbose >= 2:\n",
        "            fig_ratio = self._figsize[0] / self._figsize[1]\n",
        "            height = 5 * resolution\n",
        "            self._appearance_map_size = (int(np.round(height * fig_ratio)), height)\n",
        "            verbose_size = (self._appearance_map_size[0] + resolution, self._appearance_map_size[1])\n",
        "\n",
        "        super(FaceReenactmentRenderer, self).__init__(display, verbose, verbose_size, output_crop, resolution,\n",
        "                                                      crop_scale, encoder_codec)\n",
        "\n",
        "    def on_render(self, *args):\n",
        "        if self._verbose <= 0:\n",
        "            return tensor2bgr(args[0])\n",
        "        elif self._verbose == 1:\n",
        "            return tensor2bgr(torch.cat(args, dim=2))\n",
        "        else:\n",
        "            if self._fig is None:\n",
        "                self._fig = plt.figure(figsize=self._figsize)\n",
        "            results_bgr = tensor2bgr(torch.cat(args[:5], dim=1))\n",
        "            tgt_pose = args[5].numpy()\n",
        "            appearance_map_bgr = render_appearance_map(self._fig, self._appearance_map.tri, self._appearance_map.points,\n",
        "                                                       tgt_pose[:2])\n",
        "            appearance_map_bgr = cv2.resize(appearance_map_bgr, self._appearance_map_size,\n",
        "                                            interpolation=cv2.INTER_CUBIC)\n",
        "            render_bgr = np.concatenate((appearance_map_bgr, results_bgr), axis=1)\n",
        "            tgt_pose *= 99.     # Unnormalize the target pose for printing\n",
        "            msg = 'Pose: %.1f, %.1f, %.1f' % (tgt_pose[0], tgt_pose[1], tgt_pose[2])\n",
        "            cv2.putText(render_bgr, msg, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            return render_bgr\n",
        "\n",
        "\n",
        "def render_appearance_map(fig, tri, points, query_point=None, render_scale=99.):\n",
        "    points_scaled = points * render_scale\n",
        "    plt.triplot(points_scaled[:, 0], points_scaled[:, 1], tri.simplices.copy(), linewidth=3)\n",
        "    plt.plot(points_scaled[:, 0], points_scaled[:, 1], 'o', markersize=12)\n",
        "    if query_point is not None:\n",
        "        query_point_scaled = query_point[:2] * render_scale\n",
        "        tri_index = tri.find_simplex(query_point[:2])\n",
        "        tri_vertices = tri.simplices[tri_index]\n",
        "        plt.plot(points_scaled[tri_vertices, 0], points_scaled[tri_vertices, 1], 'yo', markersize=12)\n",
        "        plt.plot(query_point_scaled[0], query_point_scaled[1], 'rx', markersize=24, markeredgewidth=4)\n",
        "\n",
        "    plt.xlim(points_scaled[:-4, 0].min() - 0.5, points_scaled[:-4, 0].max() + 0.5)\n",
        "    plt.ylim(points_scaled[:-4, 1].min() - 0.5, points_scaled[:-4, 1].max() + 0.5)\n",
        "    plt.xlabel('Yaw (angles)', fontsize=24)\n",
        "    plt.ylabel('Pitch (angles)', fontsize=24)\n",
        "    ax = plt.gca()\n",
        "    ax.tick_params(axis='both', which='major', labelsize=24)\n",
        "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    # grab the pixel buffer and dump it into a numpy array\n",
        "    img = np.array(fig.canvas.renderer._renderer)\n",
        "    plt.clf()\n",
        "\n",
        "    return img[:, :, 2::-1]\n",
        "\n",
        "\n",
        "def select_seq(seq_list, select='longest'):\n",
        "    if select == 'longest':\n",
        "        seq = seq_list[np.argmax([len(s) for s in seq_list])]\n",
        "    elif select.isnumeric():\n",
        "        seq = seq_list[[s.id for s in seq_list].index(int(select))]\n",
        "    else:\n",
        "        raise RuntimeError(f'Unknown selection method: \"{select}\"')\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "def main(source, target, output=None, select_source=d('select_source'), select_target=d('select_target'),\n",
        "         # General arguments\n",
        "         resolution=d('resolution'), crop_scale=d('crop_scale'), gpus=d('gpus'),\n",
        "         cpu_only=d('cpu_only'), display=d('display'), verbose=d('verbose'), encoder_codec=d('encoder_codec'),\n",
        "         # Detection arguments:\n",
        "         detection_model=d('detection_model'), det_batch_size=d('det_batch_size'), det_postfix=d('det_postfix'),\n",
        "         # Sequence arguments:\n",
        "         iou_thresh=d('iou_thresh'), min_length=d('min_length'), min_size=d('min_size'),\n",
        "         center_kernel=d('center_kernel'), size_kernel=d('size_kernel'), smooth_det=d('smooth_det'),\n",
        "         seq_postfix=d('seq_postfix'), write_empty=d('write_empty'),\n",
        "         # Pose arguments:\n",
        "         pose_model=d('pose_model'), pose_batch_size=d('pose_batch_size'), pose_postfix=d('pose_postfix'),\n",
        "         cache_pose=d('cache_pose'), cache_frontal=d('cache_frontal'), smooth_poses=d('smooth_poses'),\n",
        "         # Landmarks arguments:\n",
        "         lms_model=d('lms_model'), lms_batch_size=d('lms_batch_size'), landmarks_postfix=d('landmarks_postfix'),\n",
        "         cache_landmarks=d('cache_landmarks'), smooth_landmarks=d('smooth_landmarks'),\n",
        "         # Segmentation arguments:\n",
        "         seg_model=d('seg_model'), seg_batch_size=d('seg_batch_size'), segmentation_postfix=d('segmentation_postfix'),\n",
        "         cache_segmentation=d('cache_segmentation'), smooth_segmentation=d('smooth_segmentation'),\n",
        "         seg_remove_mouth=d('seg_remove_mouth'),\n",
        "         # Finetune arguments:\n",
        "         finetune=d('finetune'), finetune_iterations=d('finetune_iterations'), finetune_lr=d('finetune_lr'),\n",
        "         finetune_batch_size=d('finetune_batch_size'), finetune_workers=d('finetune_workers'),\n",
        "         finetune_save=d('finetune_save'),\n",
        "         # Reenactment arguments:\n",
        "         batch_size=d('batch_size'), reenactment_model=d('reenactment_model'), criterion_id=d('criterion_id'),\n",
        "         min_radius=d('min_radius')):\n",
        "    face_reenactment = FaceReenactment(\n",
        "        resolution, crop_scale, gpus, cpu_only, display, verbose, encoder_codec,\n",
        "        detection_model=detection_model, det_batch_size=det_batch_size, det_postfix=det_postfix,\n",
        "        iou_thresh=iou_thresh, min_length=min_length, min_size=min_size, center_kernel=center_kernel,\n",
        "        size_kernel=size_kernel, smooth_det=smooth_det, seq_postfix=seq_postfix, write_empty=write_empty,\n",
        "        pose_model=pose_model, pose_batch_size=pose_batch_size, pose_postfix=pose_postfix,\n",
        "        cache_pose=cache_pose, cache_frontal=cache_frontal, smooth_poses=smooth_poses,\n",
        "        lms_model=lms_model, lms_batch_size=lms_batch_size, landmarks_postfix=landmarks_postfix,\n",
        "        cache_landmarks=cache_landmarks, smooth_landmarks=smooth_landmarks,\n",
        "        seg_model=seg_model, seg_batch_size=seg_batch_size, segmentation_postfix=segmentation_postfix,\n",
        "        cache_segmentation=cache_segmentation, smooth_segmentation=smooth_segmentation,\n",
        "        seg_remove_mouth=seg_remove_mouth,\n",
        "        finetune=finetune, finetune_iterations=finetune_iterations, finetune_lr=finetune_lr,\n",
        "        finetune_batch_size=finetune_batch_size, finetune_workers=finetune_workers, finetune_save=finetune_save,\n",
        "        batch_size=batch_size, reenactment_model=reenactment_model, criterion_id=criterion_id, min_radius=min_radius)\n",
        "    if len(source) == 1 and len(target) == 1 and os.path.isfile(source[0]) and os.path.isfile(target[0]):\n",
        "        face_reenactment(source[0], target[0], output, select_source, select_target)\n",
        "    else:\n",
        "        batch(source, target, output, face_reenactment, postfix='.mp4', skip_existing=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(**vars(parser.parse_args()))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting fsgan/inference/reenact.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEeE1IsG2Sw",
        "colab_type": "text"
      },
      "source": [
        "# 使用 （请确保视频素材为25fps）\n",
        "\n",
        "python swap.py SOURCE -t TARGET -o OUTPUT [FLAGS]\n",
        "\n",
        "SOURCE 为源 mp4\n",
        "\n",
        "TARGET 为目标 mp4\n",
        "\n",
        "OUTPUT 输出路径\n",
        "\n",
        "[FLAGS] 各类参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJN2z8S64m1b",
        "colab_type": "code",
        "outputId": "044fd8ac-d19c-4368-d804-095a20f53714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "# 1，生成最佳质量的视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py '/content/drive/My Drive/fsganLab/fshome/fsgan/docs/examples/shinzo_abe.mp4' -t '/content/drive/My Drive/fsganLab/fshome/fsgan/docs/examples/conan_obrien.mp4' -o . --finetune --finetune_save --seg_remove_mouth"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fsganLab/fshome\n",
            "=> using GPU devices: 0\n",
            "=> Loading face pose model: \"hopenet_robust_alpha1.pth\"...\n",
            "=> Loading face landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
            "=> Loading face segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
            "=> Loading face reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
            "=> Loading face completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
            "=> Loading face blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n",
            "=> Cropping video sequences from video: \"shinzo_abe.mp4\"...\n",
            "100% 600/600 [00:03<00:00, 195.63it/s]\n",
            "=> Computing face poses for video: \"shinzo_abe_seq00.mp4\"...\n",
            "100% 5/5 [00:03<00:00,  1.40batches/s]\n",
            "=> Computing face landmarks for video: \"shinzo_abe_seq00.mp4\"...\n",
            "100% 10/10 [00:03<00:00,  3.01batches/s]\n",
            "=> Computing face segmentation for video: \"shinzo_abe_seq00.mp4\"...\n",
            "100% 19/19 [00:06<00:00,  2.96batches/s]\n",
            "=> Detecting faces in video: \"conan_obrien.mp4...\"\n",
            "100% 150/150 [01:03<00:00,  2.37frames/s]\n",
            "=> Extracting sequences from detections in video: \"conan_obrien.mp4\"...\n",
            "100% 151/151 [00:00<00:00, 14869.58it/s]\n",
            "=> Cropping video sequences from video: \"conan_obrien.mp4\"...\n",
            "100% 150/150 [00:01<00:00, 101.20it/s]\n",
            "=> Computing face poses for video: \"conan_obrien_seq00.mp4\"...\n",
            "100% 2/2 [00:00<00:00,  2.23batches/s]\n",
            "=> Computing face landmarks for video: \"conan_obrien_seq00.mp4\"...\n",
            "100% 3/3 [00:01<00:00,  2.89batches/s]\n",
            "=> Computing face segmentation for video: \"conan_obrien_seq00.mp4\"...\n",
            "100% 5/5 [00:01<00:00,  2.81batches/s]\n",
            "=> Finetuning the reenactment generator on: \"shinzo_abe_seq00.mp4\"...\n",
            "100% 200/200 [02:48<00:00,  1.19batches/s]\n",
            "=> Face swapping: \"shinzo_abe_seq00.mp4\" -> \"conan_obrien_seq00.mp4\"...\n",
            "100% 19/19 [00:43<00:00,  2.28s/batches]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cntN0ZGir5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2，生成最佳效率的视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --seg_remove_mouth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4WqLJpvYgpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3，生成头部转后直方视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --output_crop -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YNXfFs8bsBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4，生成比较用的直方视频\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --verbose 1 -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHg5JTwib-Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5，输出所有生产信息（包括过程，主要用于调试）\n",
        "%cd /content/drive/My Drive/fsganLab/fshome\n",
        "!python swap.py ../docs/examples/shinzo_abe.mp4 -t ../docs/examples/conan_obrien.mp4 -o . --verbose 2 -f -fs -srm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvO76eTjSwdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
